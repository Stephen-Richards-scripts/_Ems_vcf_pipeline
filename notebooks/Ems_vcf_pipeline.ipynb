{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c576998-ad33-44d9-b4f9-6f0c75ba1ffe",
   "metadata": {},
   "source": [
    "# Em's VCF pipeline  \n",
    "A simple step-by-step workbook to take reads, align to a reference genome and generate per-sample vcfs or gvcf files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88872ea4-6ded-4cc5-9e93-a8204e4994d1",
   "metadata": {},
   "source": [
    "## 0. Environment Setup\n",
    "All required software is inside the conda environment:  \n",
    "**Bondlab_phylo_env**\n",
    "\n",
    "You should see this kernel selected in the top‑right of your Jupyter window.  \n",
    "If not, run the following on the FARM login node:\n",
    "\n",
    "1. module load conda  \n",
    "2. conda activate /group/jbondgrp2/stephenRichards/_conda_envs/Bondlab_phylo_env  \n",
    "3. python -m ipykernel install --user --name Bondlab_phylo_env --display-name \"Python [conda env:Bondlab_phylo_env]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0733d75c-7bdf-4dc9-8a14-21eb72677d6e",
   "metadata": {},
   "source": [
    "## 0.1 CPU Check\n",
    "The next cell checks how many CPUs are available in your OnDemand session or SLURM allocation.  \n",
    "If the number looks too small for your job, restart your Jupyter session with more CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fed0bfc-1af8-4b22-969c-4c2bb69064b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs available: 200\n"
     ]
    }
   ],
   "source": [
    "# check cpus available and set global THREADS\n",
    "import os\n",
    "THREADS = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "print(\"CPUs available:\", THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7323d0f-6775-47c2-8774-1b1f97753c7e",
   "metadata": {},
   "source": [
    "### 0.1 Python library imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5e67c7-0018-458d-9838-bceb0c54e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# UI and display\n",
    "from ipywidgets import Dropdown, Button, VBox, HTML, Output\n",
    "from IPython.display import display, clear_output, HTML, Markdown\n",
    "\n",
    "# Data + plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d1ced-d838-48af-ade8-1f67aa8ef98d",
   "metadata": {},
   "source": [
    "## 1. Input Data\n",
    "\n",
    "This pipeline requires two inputs:\n",
    "1. **A reference genome** (FASTA)\n",
    "2. **A directory of Illumina paired‑end reads**, one directory per project or batch\n",
    "\n",
    "Spider genomes are typically large (human‑sized or larger), and sequencing facilities often produce **many files**, usually:\n",
    "\n",
    "- `sampleX_R1.fastq.gz`\n",
    "- `sampleX_R2.fastq.gz`\n",
    "\n",
    "Please name your read files **something meaningful** (e.g. `Genus_species_sampleID_R1.fastq.gz`).  \n",
    "Long names are fine — meaningful names help track samples through the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34024a7-b0cc-458b-80a0-fa98bb5cbe39",
   "metadata": {},
   "source": [
    "### 1.1 Preparing Your Input Files\n",
    "Before running the pipeline, place your reference genome and read files in the expected locations on the FARM storage.  \n",
    "**1. Reference Genome**  \n",
    "If it is not alreeady there, copy your reference FASTA file into:  \n",
    "/group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/data/references/  \n",
    "\n",
    "**2. Sequencing Reads**  \n",
    "Create a new directory for your reads inside:  \n",
    "_Ems_vcf_pipeline/data/read_directories/  \n",
    "\n",
    "To save storage, **soft‑link** your FASTQ files into this directory:  \n",
    "ln -s /path/to/your/reads/*.fastq.gz   new_directory/  \n",
    "\n",
    "**3. Select Your Inputs Below**  \n",
    "The next cell will let you choose:  \n",
    "- the reference FASTA   \n",
    "- the directory containing your read files   \n",
    "\n",
    "Once selected, continue with the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a019456f-e8be-4bc3-a625-41648ef2b5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c082f934f4a5433e87b4a0531811dc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Reference:', layout=Layout(width='70%'), options=('GCA_036925085.1_qdBraP…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ipywidgets import Dropdown, Button, VBox, Output\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "out = Output()\n",
    "\n",
    "PROJECT_ROOT = \"/group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline\"\n",
    "refs_dir     = f\"{PROJECT_ROOT}/data/references\"\n",
    "reads_root   = f\"{PROJECT_ROOT}/data/read_directories\"\n",
    "\n",
    "# --- Helper functions ---\n",
    "def list_refs():\n",
    "    return sorted([f for f in os.listdir(refs_dir)\n",
    "                   if f.lower().endswith((\".fa\",\".fasta\",\".fna\"))])\n",
    "\n",
    "def list_read_dirs():\n",
    "    return sorted([d for d in os.listdir(reads_root)\n",
    "                   if os.path.isdir(os.path.join(reads_root, d))])\n",
    "\n",
    "def pair_fastqs(dirpath):\n",
    "    \"\"\"Return dict sample -> {R1,R2} and list of unpaired files.\"\"\"\n",
    "    files = sorted(Path(dirpath).glob(\"*.fastq*\"))\n",
    "    pairs = {}\n",
    "    unpaired = []\n",
    "\n",
    "    tmp = {}\n",
    "    for f in files:\n",
    "        b = f.name\n",
    "        if \"_R1\" in b:\n",
    "            stem = b.split(\"_R1\")[0]\n",
    "            tmp.setdefault(stem, {})[\"R1\"] = f\n",
    "        elif \"_R2\" in b:\n",
    "            stem = b.split(\"_R2\")[0]\n",
    "            tmp.setdefault(stem, {})[\"R2\"] = f\n",
    "        else:\n",
    "            unpaired.append(f)\n",
    "\n",
    "    for stem, rec in tmp.items():\n",
    "        if \"R1\" in rec and \"R2\" in rec:\n",
    "            pairs[stem] = rec\n",
    "        else:\n",
    "            unpaired.extend(rec.values())\n",
    "\n",
    "    return pairs, unpaired\n",
    "\n",
    "# --- Widgets ---\n",
    "ref_dd  = Dropdown(options=list_refs(), description=\"Reference:\", layout={'width':'70%'})\n",
    "reads_dd= Dropdown(options=list_read_dirs(), description=\"Reads:\", layout={'width':'70%'})\n",
    "btn     = Button(description=\"Select\", button_style=\"success\")\n",
    "\n",
    "display(VBox([ref_dd, reads_dd, btn, out]))\n",
    "\n",
    "def safe_symlink(target: Path, link_path: Path):\n",
    "    \"\"\"Create or refresh a symlink.\"\"\"\n",
    "    link_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if link_path.exists() or link_path.is_symlink():\n",
    "        try:\n",
    "            if link_path.is_symlink() and link_path.resolve() == target.resolve():\n",
    "                return  # already correct\n",
    "            link_path.unlink()\n",
    "        except Exception:\n",
    "            # As a fallback, rename to .bak and continue\n",
    "            link_path.rename(link_path.with_suffix(link_path.suffix + \".bak\"))\n",
    "    link_path.symlink_to(target)\n",
    "\n",
    "def on_click(_):\n",
    "    clear_output(wait=True)\n",
    "    display(VBox([ref_dd, reads_dd, btn]))\n",
    "\n",
    "    ref_src = Path(refs_dir) / ref_dd.value         # canonical source FASTA\n",
    "    rd      = Path(reads_root) / reads_dd.value\n",
    "\n",
    "    pairs, unpaired = pair_fastqs(rd)\n",
    "\n",
    "    # Create run directory and reference view\n",
    "    run       = Path(PROJECT_ROOT) / \"results\" / reads_dd.value\n",
    "    ref_work  = run / \"reference\"\n",
    "    ref_work.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Symlink FASTA into the run/reference dir\n",
    "    ref_link = ref_work / ref_src.name\n",
    "    safe_symlink(ref_src, ref_link)\n",
    "\n",
    "    # ---- Save to globals ----\n",
    "    # Keep both the source FASTA and the symlink path:\n",
    "    # - REF_FASTA_SRC: where indexes will be built and live (permanent)\n",
    "    # - REF_FASTA:     a convenience alias used throughout (points to the symlink in run/reference)\n",
    "    global REFERENCE, READS_DIR, PAIRS, UNPAIRED, RUN_DIR, REF_FASTA, REF_FASTA_SRC\n",
    "    REFERENCE     = str(ref_link)     # maintained for backward compatibility with your later cells\n",
    "    REF_FASTA     = REFERENCE         # alias used later\n",
    "    REF_FASTA_SRC = str(ref_src)      # canonical source FASTA (index location)\n",
    "    READS_DIR     = str(rd)\n",
    "    PAIRS         = pairs\n",
    "    UNPAIRED      = unpaired\n",
    "    RUN_DIR       = str(run)\n",
    "\n",
    "    # Summary\n",
    "    with out:\n",
    "        print(\"✓ Reference:\", ref_src.name)\n",
    "        print(\"✓ Read set:\", reads_dd.value)\n",
    "        print(\"✓ Samples detected:\", len(pairs))\n",
    "        if unpaired:\n",
    "            print(\"⚠ Unpaired files:\", len(unpaired))\n",
    "        print(\"Run directory:\", run)\n",
    "        print(\"Reference symlink in run:\", ref_link)\n",
    "\n",
    "btn.on_click(on_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce70a1-ecfd-46d3-98bf-16b979f3ed7b",
   "metadata": {},
   "source": [
    "## 2. Pipeline preparation: \n",
    "### 2.1 Index reference files (if necessary):  \n",
    "This takes a few minutes to run, if the index files have not already been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e035ab-cb6b-42fc-bc12-d3ecff424f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference FASTA (source): /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/data/references/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa\n",
      "Run reference dir (symlinks): /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference\n",
      "\n",
      "[1/2] samtools faidx\n",
      "  .fai index exists in source, skipping.\n",
      "\n",
      "[2/2] bwa-mem2 index\n",
      "  bwa-mem2 index exists in source, skipping.\n",
      "\n",
      "Linking into run/reference:\n",
      "  -> /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa\n",
      "  -> /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa.fai\n",
      "  -> /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa.0123\n",
      "  -> /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa.amb\n",
      "  -> /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa.ann\n",
      "  -> /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa.bwt.2bit.64\n",
      "  -> /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/reference/GCA_036925085.1_qdBraProd1.0.pri_genomic.fa.pac\n",
      "\n",
      "Done. Reference indexes live in source; run dir has symlinks.\n"
     ]
    }
   ],
   "source": [
    "## 2. Pipeline preparation: Index reference files (if necessary)\n",
    "import os, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Require globals from selection cell ----\n",
    "need = {\"REF_FASTA\", \"REF_FASTA_SRC\", \"RUN_DIR\"}\n",
    "missing = [k for k in need if k not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"{', '.join(missing)} not set. Run the selection cell first.\")\n",
    "\n",
    "REF_FASTA_SRC = Path(REF_FASTA_SRC)             # canonical FASTA in refs_dir\n",
    "REF_FASTA_LINK_DIR = Path(RUN_DIR) / \"reference\" # where we keep symlinks\n",
    "REF_FASTA_LINK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run(cmd):\n",
    "    print(\"  $\", \" \".join(map(str, cmd)))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "def safe_symlink(target: Path, link_path: Path):\n",
    "    link_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if link_path.exists() or link_path.is_symlink():\n",
    "        try:\n",
    "            if link_path.is_symlink() and link_path.resolve() == target.resolve():\n",
    "                return\n",
    "            link_path.unlink()\n",
    "        except Exception:\n",
    "            link_path.rename(link_path.with_suffix(link_path.suffix + \".bak\"))\n",
    "    link_path.symlink_to(target)\n",
    "\n",
    "fasta_src = str(REF_FASTA_SRC)\n",
    "print(\"Reference FASTA (source):\", fasta_src)\n",
    "print(\"Run reference dir (symlinks):\", REF_FASTA_LINK_DIR)\n",
    "\n",
    "# ---- Expected index files for samtools & bwa-mem2 ----\n",
    "# samtools faidx:\n",
    "fai_src = REF_FASTA_SRC.with_suffix(REF_FASTA_SRC.suffix + \".fai\")\n",
    "\n",
    "# bwa-mem2 creates a set of files next to the FASTA; the common set is:\n",
    "#   .0123  .amb  .ann  .bwt.2bit.64  .pac\n",
    "bwa_exts = [\".0123\", \".amb\", \".ann\", \".bwt.2bit.64\", \".pac\"]\n",
    "bwa_src_files = [REF_FASTA_SRC.with_suffix(REF_FASTA_SRC.suffix + ext) for ext in bwa_exts]\n",
    "\n",
    "print(\"\\n[1/2] samtools faidx\")\n",
    "if not fai_src.exists():\n",
    "    print(\"  Creating .fai index in source directory\")\n",
    "    run([\"samtools\", \"faidx\", fasta_src])\n",
    "else:\n",
    "    print(\"  .fai index exists in source, skipping.\")\n",
    "\n",
    "print(\"\\n[2/2] bwa-mem2 index\")\n",
    "# If any expected file is missing, (re)build\n",
    "if not all(p.exists() for p in bwa_src_files):\n",
    "    print(\"  Building bwa-mem2 index in source directory\")\n",
    "    # If you want to parallelize indexing, you can add threads via env var for zlib (less helpful)\n",
    "    # or simply run the default command; bwa-mem2 index is generally fast enough.\n",
    "    run([\"bwa-mem2\", \"index\", fasta_src])\n",
    "    # Refresh the list in case filenames vary by version\n",
    "    bwa_src_files = [REF_FASTA_SRC.with_suffix(REF_FASTA_SRC.suffix + ext) for ext in bwa_exts]\n",
    "else:\n",
    "    print(\"  bwa-mem2 index exists in source, skipping.\")\n",
    "\n",
    "# ---- Ensure symlinks to FASTA and all index files in the run/reference dir ----\n",
    "print(\"\\nLinking into run/reference:\")\n",
    "# Link the FASTA itself\n",
    "fasta_link = REF_FASTA_LINK_DIR / REF_FASTA_SRC.name\n",
    "safe_symlink(REF_FASTA_SRC, fasta_link)\n",
    "print(\"  ->\", fasta_link)\n",
    "\n",
    "# Link samtools faidx\n",
    "fai_link = REF_FASTA_LINK_DIR / (REF_FASTA_SRC.name + \".fai\")\n",
    "safe_symlink(fai_src, fai_link)\n",
    "print(\"  ->\", fai_link)\n",
    "\n",
    "# Link bwa-mem2 index files that actually exist\n",
    "for p in bwa_src_files:\n",
    "    if p.exists():\n",
    "        link_p = REF_FASTA_LINK_DIR / p.name\n",
    "        safe_symlink(p, link_p)\n",
    "        print(\"  ->\", link_p)\n",
    "\n",
    "print(\"\\nDone. Reference indexes live in source; run dir has symlinks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad779d5-fd3e-4024-8f5a-754b1f27e7a3",
   "metadata": {},
   "source": [
    "## 3. Pipeline: QC on raw reads\n",
    "### 3.1 Runs FastQC on raw R1/R2 reads for each sample.  \n",
    "If reports already exist, this cell will skip them.  \n",
    "(Optionally run the next cell to build a MultiQC summary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e498fe-d1c0-41d1-91db-1c8da4c239b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FastQC on raw reads: 10 sample(s) (to do=10, skipped=0)\n",
      "Scheduling 10 FastQC worker(s), 2 thread(s) each (<= 200 total threads)\n",
      "[Progress] 10/10 | ETA=0:00:00   \n",
      "\n",
      "Raw FastQC finished.\n",
      "Outputs: /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/qc/fastqc_raw\n",
      "Summary: wrote=10, skipped=0, failed=0, total=10\n"
     ]
    }
   ],
   "source": [
    "## 3. QC on raw reads (FastQC) — parallel by sample\n",
    "from pathlib import Path\n",
    "import os, re, shutil, subprocess, concurrent.futures, time\n",
    "from datetime import timedelta\n",
    "\n",
    "# ---- Required globals from earlier cells ----\n",
    "for name in [\"RUN_DIR\", \"PAIRS\", \"THREADS\"]:\n",
    "    if name not in globals() or globals()[name] in (None, \"\", {}):\n",
    "        raise RuntimeError(f\"Missing required global: {name}. Run the selection cell first.\")\n",
    "\n",
    "# Tool check\n",
    "if shutil.which(\"fastqc\") is None:\n",
    "    raise RuntimeError(\"fastqc not found in PATH.\")\n",
    "\n",
    "# Output dir\n",
    "global QC_RAW_DIR\n",
    "QC_RAW_DIR = Path(RUN_DIR) / \"qc\" / \"fastqc_raw\"\n",
    "QC_RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run(cmd):\n",
    "    print(\"  $\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "def fastqc_report_html(outdir: Path, fq_path: str) -> Path:\n",
    "    base = Path(fq_path).name\n",
    "    base = re.sub(r'\\.(fastq|fq)(\\.gz)?$', '', base, flags=re.IGNORECASE)\n",
    "    return outdir / f\"{base}_fastqc.html\"\n",
    "\n",
    "if not PAIRS:\n",
    "    print(\"No paired FASTQs found (PAIRS is empty). Did you select the correct read directory?\")\n",
    "else:\n",
    "    # ---------- Build worklist, skip if both R1/R2 HTMLs exist ----------\n",
    "    jobs = []\n",
    "    skipped = 0\n",
    "    for sample_id, paths in sorted(PAIRS.items()):\n",
    "        r1, r2 = str(paths[\"R1\"]), str(paths[\"R2\"])\n",
    "        missing = [p for p in (r1, r2) if not os.path.exists(p)]\n",
    "        if missing:\n",
    "            print(f\"[WARN] Skipping {sample_id} — missing file(s): {', '.join(missing)}\")\n",
    "            continue\n",
    "\n",
    "        r1_html = fastqc_report_html(QC_RAW_DIR, r1)\n",
    "        r2_html = fastqc_report_html(QC_RAW_DIR, r2)\n",
    "        if r1_html.exists() and r2_html.exists():\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        jobs.append((sample_id, r1, r2))\n",
    "\n",
    "    total_samples = len(PAIRS)\n",
    "    todo = len(jobs)\n",
    "    print(f\"Running FastQC on raw reads: {total_samples} sample(s) \"\n",
    "          f\"(to do={todo}, skipped={skipped})\")\n",
    "\n",
    "    if todo == 0:\n",
    "        print(\"Nothing to do.\\nRaw FastQC outputs are already present at:\", QC_RAW_DIR)\n",
    "    else:\n",
    "        # ---------- Simple concurrency policy ----------\n",
    "        PER_JOB = 2              # fixed FastQC threads per sample\n",
    "        THREADS = int(THREADS)\n",
    "\n",
    "        # workers * PER_JOB <= THREADS\n",
    "        workers = max(1, min(todo, THREADS // PER_JOB))\n",
    "\n",
    "        print(f\"Scheduling {workers} FastQC worker(s), {PER_JOB} thread(s) each \"\n",
    "            f\"(<= {THREADS} total threads)\")\n",
    "\n",
    "        # ---------- Worker ----------\n",
    "        def work_one(sample_id: str, r1: str, r2: str):\n",
    "            # FastQC can take both mates at once; -t applies to the process\n",
    "            cmd = [\"fastqc\", \"-t\", str(PER_JOB), \"-o\", str(QC_RAW_DIR), r1, r2]\n",
    "            # We don't print every command to keep output tidy; uncomment if needed:\n",
    "            # print(\"  $\", \" \".join(cmd))\n",
    "            res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if res.returncode != 0:\n",
    "                raise RuntimeError(res.stderr.strip() or \"fastqc failed\")\n",
    "\n",
    "        # ---------- Run pool with basic progress ----------\n",
    "        start = time.time()\n",
    "        done = 0\n",
    "        failures = []\n",
    "\n",
    "        def eta_text(d, t, elapsed):\n",
    "            if d <= 0: return \"--:--:--\"\n",
    "            rate = d / max(elapsed, 1e-6)\n",
    "            rem = t - d\n",
    "            secs = int(rem / rate) if rate > 0 else 0\n",
    "            return str(timedelta(seconds=secs))\n",
    "\n",
    "        print(f\"[Progress] 0/{todo} | ETA=--:--:--\", end=\"\", flush=True)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "            futmap = {ex.submit(work_one, sid, r1, r2): sid for sid, r1, r2 in jobs}\n",
    "            for fut in concurrent.futures.as_completed(futmap):\n",
    "                sid = futmap[fut]\n",
    "                try:\n",
    "                    fut.result()\n",
    "                except Exception as e:\n",
    "                    failures.append((sid, str(e)))\n",
    "                finally:\n",
    "                    done += 1\n",
    "                    eta = eta_text(done, todo, time.time() - start)\n",
    "                    print(f\"\\r[Progress] {done}/{todo} | ETA={eta}   \", end=\"\", flush=True)\n",
    "\n",
    "        print()  # newline after progress\n",
    "\n",
    "        # ---------- Summary ----------\n",
    "        wrote = todo - len(failures)\n",
    "        print(\"\\nRaw FastQC finished.\")\n",
    "        print(\"Outputs:\", QC_RAW_DIR)\n",
    "        print(f\"Summary: wrote={wrote}, skipped={skipped}, failed={len(failures)}, total={total_samples}\")\n",
    "        if failures:\n",
    "            print(\"Failed samples:\")\n",
    "            for sid, msg in failures:\n",
    "                print(f\"  - {sid}: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69451975-70b4-47c3-9b37-7dc5673f3d76",
   "metadata": {},
   "source": [
    "### 3.2 MultiQC\n",
    "After this next MultiQC cell is run it is best to downlaod the html file to your laptop and open in a regular browser with javascript to see the charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aab4d64-3b70-4a65-a27c-e8398d419834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MultiQC report for raw FastQC…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[91m///\u001b[0m \u001b]8;id=514787;https://multiqc.info\u001b\\\u001b[1mMultiQC\u001b[0m\u001b]8;;\u001b\\ \u001b[2mv1.33\u001b[0m\n",
      "\n",
      "\u001b[34m       file_search\u001b[0m | Search path: /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/qc/fastqc_raw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m        searching \u001b[0m| ████████████████████████████████████████ 100% 40/40                                                   html\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m            fastqc\u001b[0m | Found 20 reports\n",
      "\u001b[34m     write_results\u001b[0m | Data        : /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/qc/fastqc_raw/multiqc_data\n",
      "\u001b[34m     write_results\u001b[0m | Report      : /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/qc/fastqc_raw/multiqc_report.html\n",
      "\u001b[34m           multiqc\u001b[0m | MultiQC complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. See: /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/qc/fastqc_raw/multiqc_report.html\n"
     ]
    }
   ],
   "source": [
    "## 3A. (Optional) MultiQC summary of raw FastQC reports\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "if \"QC_RAW_DIR\" not in globals():\n",
    "    raise RuntimeError(\"QC_RAW_DIR is not set. Run the FastQC cell first.\")\n",
    "\n",
    "print(\"Building MultiQC report for raw FastQC…\")\n",
    "subprocess.run([\"multiqc\", str(QC_RAW_DIR), \"-o\", str(QC_RAW_DIR)], check=True)\n",
    "print(\"Done. See:\", Path(QC_RAW_DIR) / \"multiqc_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081c8f25-4d12-4ed6-9c2f-f00f81caeaa6",
   "metadata": {},
   "source": [
    "## 4. Trimming adapters with cutadapt  \n",
    "I am assuming the sequence came of a Novaseq or similar 2 color illumina machine for this pipeline (I will need to make a different pipeline for different data.) So I have the flag   --nextseq-trim=20 \\          # specialized 3′ poly-G trim in the trimming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c47df8-af80-425b-b0af-7d818aa4b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming with cutadapt (THREADS=200)\n",
      "Samples: 10 (to do=10, skipped=0)\n",
      "Scheduling 10 cutadapt worker(s), 6 thread(s) each (<= 200 total threads)\n",
      "[Progress] 10/10 | ETA=0:00:00   \n",
      "\n",
      "Trim completed.\n",
      "Outputs: /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/trimmed\n",
      "Summary: wrote=10, skipped=0, failed=0, total=10\n"
     ]
    }
   ],
   "source": [
    "## 4. Trimming with cutadapt — parallel by sample\n",
    "from pathlib import Path\n",
    "import os, re, shutil, subprocess, concurrent.futures, time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Globals needed: RUN_DIR, PAIRS, THREADS, PROJECT_ROOT\n",
    "for name in [\"RUN_DIR\", \"PAIRS\", \"THREADS\", \"PROJECT_ROOT\"]:\n",
    "    if name not in globals() or not globals()[name]:\n",
    "        raise RuntimeError(f\"Missing global: {name}\")\n",
    "\n",
    "# Tool check\n",
    "if shutil.which(\"cutadapt\") is None:\n",
    "    raise RuntimeError(\"cutadapt not found in PATH.\")\n",
    "\n",
    "TRIM_DIR = Path(RUN_DIR) / \"trimmed\"\n",
    "TRIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ADAPTERS = Path(PROJECT_ROOT) / \"data\" / \"adapters\" / \"adapters.fasta\"\n",
    "\n",
    "def print_adapters_file_missing_error_message():\n",
    "    print(\"\"\"\n",
    "adapters file missing. Please create a FASTA file at:\n",
    "    <PROJECT_ROOT>/data/adapters/adapters.fasta\n",
    "Example:\n",
    ">i5\n",
    "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTCCTATTGAGTGTAGATCTCGGTGGTCGCCGTATCATT\n",
    ">i7\n",
    "GATCGGAAGAGCACACGTCTGAACTCCAGTCACCGTGCTGGATCTCGTATGCCGTCTTCTGCTTG\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Trimming with cutadapt (THREADS={THREADS})\")\n",
    "\n",
    "if not PAIRS:\n",
    "    print(\"No paired FASTQs found (PAIRS is empty). Did you select the correct read directory?\")\n",
    "else:\n",
    "    # Infer output extension from the first R1\n",
    "    first_r1 = next(iter(PAIRS.values()))[\"R1\"]\n",
    "    gz = str(first_r1).lower().endswith(\".gz\")\n",
    "    EXT = \".fastq.gz\" if gz else \".fastq\"\n",
    "\n",
    "    # ---------- Build worklist, skipping samples with both outputs present ----------\n",
    "    jobs = []\n",
    "    skipped = 0\n",
    "    for sample, paths in sorted(PAIRS.items()):\n",
    "        r1 = str(paths[\"R1\"])\n",
    "        r2 = str(paths[\"R2\"])\n",
    "\n",
    "        out1 = TRIM_DIR / f\"{sample}.R1.trimmed{EXT}\"\n",
    "        out2 = TRIM_DIR / f\"{sample}.R2.trimmed{EXT}\"\n",
    "\n",
    "        # Input existence check\n",
    "        missing = [p for p in (r1, r2) if not os.path.exists(p)]\n",
    "        if missing:\n",
    "            print(f\"[WARN] Skipping {sample} — missing file(s): {', '.join(missing)}\")\n",
    "            continue\n",
    "\n",
    "        # Idempotency\n",
    "        if out1.exists() and out2.exists():\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        jobs.append((sample, r1, r2, out1, out2))\n",
    "\n",
    "    total = len(PAIRS)\n",
    "    todo = len(jobs)\n",
    "    print(f\"Samples: {total} (to do={todo}, skipped={skipped})\")\n",
    "\n",
    "    if todo == 0:\n",
    "        print(\"Nothing to trim.\\nOutputs already present in:\", TRIM_DIR)\n",
    "    else:\n",
    "        # ---------- Concurrency policy (simple, safe) ----------\n",
    "        THREADS = int(THREADS)\n",
    "        PER_JOB = int(globals().get(\"FASTQ_TRIM_THREADS_PER_SAMPLE\", 6))  # set to 2 or 4; 4 is a good default\n",
    "        if PER_JOB < 1:\n",
    "            PER_JOB = 1\n",
    "        if PER_JOB > THREADS:\n",
    "            PER_JOB = THREADS\n",
    "        workers = max(1, min(todo, THREADS // PER_JOB))\n",
    "        print(f\"Scheduling {workers} cutadapt worker(s), {PER_JOB} thread(s) each (<= {THREADS} total threads)\")\n",
    "\n",
    "        # ---------- Worker ----------\n",
    "        def trim_one(sample: str, r1: str, r2: str, out1: Path, out2: Path):\n",
    "            # Base cutadapt arguments:\n",
    "            # -j: threads for this process\n",
    "            # -q: quality trimming (5' and 3' per read)\n",
    "            # -m: minimum length\n",
    "            # --pair-filter=any: drop pair if any mate falls below length/filters (good default)\n",
    "            # --trim-n: trim terminal Ns\n",
    "            cmd = [\n",
    "                \"cutadapt\",\n",
    "                \"-j\", str(PER_JOB),\n",
    "                \"-q\", \"15,15\",\n",
    "                \"-m\", \"36\",\n",
    "                \"--pair-filter=any\",\n",
    "                \"--trim-n\",\n",
    "                \"--nextseq-trim=20\",   # specialized 3' poly-G trim for 2 color chemistry\n",
    "                \"-o\", str(out1),\n",
    "                \"-p\", str(out2),\n",
    "            ]\n",
    "            if ADAPTERS.exists():\n",
    "                # Provide adapters for 5' and 3' of both mates\n",
    "                cmd += [\"-g\", f\"file:{ADAPTERS}\", \"-G\", f\"file:{ADAPTERS}\",\n",
    "                        \"-a\", f\"file:{ADAPTERS}\", \"-A\", f\"file:{ADAPTERS}\"]\n",
    "            else:\n",
    "                # Still run without adapters, but warn once per job\n",
    "                print_adapters_file_missing_error_message()\n",
    "\n",
    "            cmd += [r1, r2]\n",
    "            # Capture stderr to surface any errors; cutadapt writes its report to stderr by default\n",
    "            res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if res.returncode != 0:\n",
    "                raise RuntimeError(res.stderr.strip() or \"cutadapt failed\")\n",
    "            # Optional: write the cutadapt report next to outputs\n",
    "            report_path = TRIM_DIR / f\"{sample}.cutadapt.txt\"\n",
    "            with open(report_path, \"w\") as rf:\n",
    "                rf.write(res.stderr)\n",
    "\n",
    "        # ---------- Run pool with progress ----------\n",
    "        start = time.time()\n",
    "        done = 0\n",
    "        failures = []\n",
    "\n",
    "        def eta_text(d, t, elapsed):\n",
    "            if d <= 0: return \"--:--:--\"\n",
    "            rate = d / max(elapsed, 1e-6)\n",
    "            rem = t - d\n",
    "            secs = int(rem / rate) if rate > 0 else 0\n",
    "            return str(timedelta(seconds=secs))\n",
    "\n",
    "        print(f\"[Progress] 0/{todo} | ETA=--:--:--\", end=\"\", flush=True)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "            futmap = {ex.submit(trim_one, s, r1, r2, o1, o2): s for (s, r1, r2, o1, o2) in jobs}\n",
    "            for fut in concurrent.futures.as_completed(futmap):\n",
    "                sid = futmap[fut]\n",
    "                try:\n",
    "                    fut.result()\n",
    "                except Exception as e:\n",
    "                    failures.append((sid, str(e)))\n",
    "                finally:\n",
    "                    done += 1\n",
    "                    print(f\"\\r[Progress] {done}/{todo} | ETA={eta_text(done, todo, time.time()-start)}   \", end=\"\", flush=True)\n",
    "\n",
    "        print()  # newline\n",
    "        wrote = todo - len(failures)\n",
    "        print(\"\\nTrim completed.\")\n",
    "        print(\"Outputs:\", TRIM_DIR)\n",
    "        print(f\"Summary: wrote={wrote}, skipped={skipped}, failed={len(failures)}, total={total}\")\n",
    "        if failures:\n",
    "            print(\"Failed samples:\")\n",
    "            for sid, msg in failures:\n",
    "                print(f\"  - {sid}: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900710a0-6c68-4215-bf36-ef2e0a0536ec",
   "metadata": {},
   "source": [
    "## 5. Align | Sort, Mark Duplicates, Index and Cleanup  \n",
    "It will take some time to create the bams as they are streaming to the sort, so you won't see the intermediate files (to save space and time). This is obviously a slow bit doing the alignment work - give it some cores.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b08a481-5d67-4ef4-b8e9-7cfa40a90585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Align→Sort→Dedup for 10 sample(s) (to do=10, skipped=0)\n",
      "Concurrency: workers=6 | PER_SAMPLE_THREADS=32 (BWA=32, SORT=12, DEDUP=24) | THREADS(total)=200\n",
      "[Progress] 0/10 | ETA=--:--:--[CCGPMC021_BMEA101919_S26_L004] Align+Sort: ✓ in 0:04:57\n",
      "[CCGPMC021_BMEA101919_S26_L004] MarkDup(+Index): ✓ in 0:01:54\n",
      "[CCGPMC021_BMEA101919_S26_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 1/10 | ETA=1:01:55   [CCGPMC021_BMEA101904_S1_L004] Align+Sort: ✓ in 0:14:26\n",
      "[CCGPMC021_BMEA101907_S2_L004] Align+Sort: ✓ in 0:16:19\n",
      "[CCGPMC021_BMEA101904_S1_L004] MarkDup(+Index): ✓ in 0:03:48\n",
      "[CCGPMC021_BMEA101904_S1_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 2/10 | ETA=1:12:58   [CCGPMC021_BMEA101907_S2_L004] MarkDup(+Index): ✓ in 0:04:42\n",
      "[CCGPMC021_BMEA101907_S2_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 3/10 | ETA=0:49:05   [CCGPMC021_BMEA101923_S27_L004] Align+Sort: ✓ in 0:30:59\n",
      "[CCGPMC021_BMEA101912_S25_L004] Align+Sort: ✓ in 0:33:42\n",
      "[CCGPMC021_BMEA101923_S27_L004] MarkDup(+Index): ✓ in 0:02:58\n",
      "[CCGPMC021_BMEA101923_S27_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 4/10 | ETA=0:50:57   [CCGPMC021_BMEA101912_S25_L004] MarkDup(+Index): ✓ in 0:02:37\n",
      "[CCGPMC021_BMEA101912_S25_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 5/10 | ETA=0:36:20   [CCGPMC021_BMEA101935_S30_L004] Align+Sort: ✓ in 0:29:27\n",
      "[CCGPMC021_BMEA101935_S30_L004] MarkDup(+Index): ✓ in 0:03:07\n",
      "[CCGPMC021_BMEA101935_S30_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 6/10 | ETA=0:33:53   [CCGPMC021_BMEA101941_S31_L004] Align+Sort: ✓ in 0:31:33\n",
      "[CCGPMC021_BMEA101941_S31_L004] MarkDup(+Index): ✓ in 0:02:10\n",
      "[CCGPMC021_BMEA101941_S31_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 7/10 | ETA=0:23:28   [CCGPMC021_BMEA101927_S28_L004] Align+Sort: ✓ in 0:57:09\n",
      "[CCGPMC021_BMEA101931_S29_L004] Align+Sort: ✓ in 0:51:20\n",
      "[CCGPMC021_BMEA101945_S32_L004] Align+Sort: ✓ in 0:25:24\n",
      "[CCGPMC021_BMEA101927_S28_L004] MarkDup(+Index): ✓ in 0:02:35\n",
      "[CCGPMC021_BMEA101927_S28_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 8/10 | ETA=0:14:56   [CCGPMC021_BMEA101931_S29_L004] MarkDup(+Index): ✓ in 0:02:41\n",
      "[CCGPMC021_BMEA101931_S29_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 9/10 | ETA=0:06:46   [CCGPMC021_BMEA101945_S32_L004] MarkDup(+Index): ✓ in 0:01:43\n",
      "[CCGPMC021_BMEA101945_S32_L004] Cleanup: removed temp sorted BAM\n",
      "[Progress] 10/10 | ETA=0:00:00   \n",
      "\n",
      "Alignment+Dedup complete.\n",
      "Outputs: /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/results/Brachycybe_producta_10sample_readset/alignments\n",
      "Summary: wrote=10, skipped=0, failed=0, total=10\n"
     ]
    }
   ],
   "source": [
    "## 5. Align → Sort (streamed) → MarkDuplicates(+index) → Cleanup  [quiet logs + concise console]\n",
    "from pathlib import Path\n",
    "import os, shutil, subprocess, concurrent.futures, time\n",
    "from datetime import timedelta\n",
    "\n",
    "# ===== Required globals =====\n",
    "for name in [\"RUN_DIR\", \"TRIM_DIR\", \"REF_FASTA\", \"PAIRS\", \"THREADS\"]:\n",
    "    if name not in globals() or not globals()[name]:\n",
    "        raise RuntimeError(f\"Missing global: {name}\")\n",
    "\n",
    "RUN_DIR   = Path(RUN_DIR)\n",
    "TRIM_DIR  = Path(TRIM_DIR)\n",
    "REF_FASTA = Path(REF_FASTA)\n",
    "\n",
    "ALN_DIR   = RUN_DIR / \"alignments\"\n",
    "LOGS_DIR  = ALN_DIR / \"logs\"\n",
    "ALN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "READ_SET_NAME = globals().get(\"READ_SET_NAME\", None)\n",
    "\n",
    "# ===== Tool availability =====\n",
    "def _which_or_raise(name):\n",
    "    if shutil.which(name) is None:\n",
    "        raise RuntimeError(f\"'{name}' not found in PATH.\")\n",
    "_which_or_raise(\"bwa-mem2\")\n",
    "_which_or_raise(\"samtools\")\n",
    "\n",
    "DEDUP_TOOL = globals().get(\"DEDUP_TOOL\", \"spark\").lower()\n",
    "if DEDUP_TOOL in (\"classic\", \"spark\"):\n",
    "    _which_or_raise(\"gatk\")\n",
    "elif DEDUP_TOOL == \"sambamba\":\n",
    "    _which_or_raise(\"sambamba\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Unknown DEDUP_TOOL='{DEDUP_TOOL}'. Use 'classic', 'spark', or 'sambamba'.\")\n",
    "\n",
    "# ===== Thread policy (per-sample) =====\n",
    "BWA_THREADS   = int(globals().get(\"BWA_THREADS\", 32))\n",
    "SORT_THREADS  = int(globals().get(\"SORT_THREADS\", 12))\n",
    "DEDUP_THREADS = int(globals().get(\"DEDUP_THREADS\", 24))\n",
    "PER_SAMPLE_THREADS = int(globals().get(\"PER_SAMPLE_THREADS\", max(BWA_THREADS, SORT_THREADS, DEDUP_THREADS)))\n",
    "THREADS = int(THREADS)\n",
    "if PER_SAMPLE_THREADS < 1:\n",
    "    PER_SAMPLE_THREADS = 1\n",
    "if PER_SAMPLE_THREADS > THREADS:\n",
    "    PER_SAMPLE_THREADS = THREADS\n",
    "\n",
    "def _calc_workers(todo:int) -> int:\n",
    "    return max(1, min(todo, THREADS // PER_SAMPLE_THREADS if PER_SAMPLE_THREADS else 1))\n",
    "\n",
    "# ===== Utilities =====\n",
    "def log_path(sample: str) -> Path:\n",
    "    return LOGS_DIR / f\"{sample}.align_dedup.log\"\n",
    "\n",
    "def append_log(p: Path, text: str):\n",
    "    with open(p, \"a\") as fh:\n",
    "        fh.write(text)\n",
    "        if not text.endswith(\"\\n\"):\n",
    "            fh.write(\"\\n\")\n",
    "\n",
    "def run_silently(cmd, sample: str, use_shell=False, env=None) -> tuple[int, str, str]:\n",
    "    \"\"\"\n",
    "    Run a command, capture stdout/stderr, append both to the per-sample log.\n",
    "    Return (returncode, stdout, stderr).\n",
    "    \"\"\"\n",
    "    lp = log_path(sample)\n",
    "    append_log(lp, f\"$ {' '.join(cmd) if isinstance(cmd, list) else cmd}\")\n",
    "    try:\n",
    "        if use_shell:\n",
    "            res = subprocess.run(cmd, shell=True, check=False, executable=\"/bin/bash\",\n",
    "                                 env=env, capture_output=True, text=True)\n",
    "        else:\n",
    "            res = subprocess.run(cmd, check=False, env=env, capture_output=True, text=True)\n",
    "    except Exception as e:\n",
    "        append_log(lp, f\"[exec error] {e}\")\n",
    "        return (1, \"\", str(e))\n",
    "\n",
    "    if res.stdout:\n",
    "        append_log(lp, res.stdout)\n",
    "    if res.stderr:\n",
    "        append_log(lp, res.stderr)\n",
    "    return (res.returncode, res.stdout, res.stderr)\n",
    "\n",
    "def trimmed_read(sample, tag):\n",
    "    gz = TRIM_DIR / f\"{sample}.{tag}.trimmed.fastq.gz\"\n",
    "    fq = TRIM_DIR / f\"{sample}.{tag}.trimmed.fastq\"\n",
    "    return gz if gz.exists() else fq\n",
    "\n",
    "def rg_string(sample: str):\n",
    "    LB = READ_SET_NAME if READ_SET_NAME else sample\n",
    "    return f'@RG\\\\tID:{sample}\\\\tSM:{sample}\\\\tPL:ILLUMINA\\\\tLB:{LB}\\\\tPU:{sample}'\n",
    "\n",
    "# ===== Per-sample pipeline =====\n",
    "def process_one_sample(sample: str):\n",
    "    t0 = time.time()\n",
    "    lp = log_path(sample)\n",
    "    append_log(lp, f\"=== {sample} ===\")\n",
    "\n",
    "    r1 = trimmed_read(sample, \"R1\")\n",
    "    r2 = trimmed_read(sample, \"R2\")\n",
    "    if not r1.exists() or not r2.exists():\n",
    "        return (sample, False, \"missing trimmed reads\")\n",
    "\n",
    "    bam_sorted = ALN_DIR / f\"{sample}.sorted.bam\"\n",
    "    bam_dedup  = ALN_DIR / f\"{sample}.dedup.bam\"\n",
    "    bai_dedup  = ALN_DIR / f\"{sample}.dedup.bam.bai\"\n",
    "    metrics    = ALN_DIR / f\"{sample}.metrics.txt\"\n",
    "\n",
    "    # 1) Align+Sort (quiet; logs captured)\n",
    "    if bam_sorted.exists():\n",
    "        print(f\"[{sample}] Align+Sort: ✓ (exists)\")\n",
    "    else:\n",
    "        t1 = time.time()\n",
    "        # Quiet bwa-mem2 with -v 1; capture everything to log\n",
    "        cmd = (\n",
    "            \"set -o pipefail; \"\n",
    "            f\"bwa-mem2 mem -t {BWA_THREADS} -v 1 -R \\\"{rg_string(sample)}\\\" \\\"{REF_FASTA}\\\" \\\"{r1}\\\" \\\"{r2}\\\" \"\n",
    "            f\"| samtools sort -@ {SORT_THREADS} -o \\\"{bam_sorted}\\\"\"\n",
    "        )\n",
    "        rc, _, _ = run_silently(cmd, sample, use_shell=True)\n",
    "        dt = timedelta(seconds=int(time.time() - t1))\n",
    "        if rc == 0 and bam_sorted.exists():\n",
    "            print(f\"[{sample}] Align+Sort: ✓ in {dt}\")\n",
    "        else:\n",
    "            return (sample, False, f\"Align+Sort failed (see log: {lp.name})\")\n",
    "\n",
    "    # 2) Mark duplicates + index (quiet)\n",
    "    if bam_dedup.exists() and bai_dedup.exists():\n",
    "        print(f\"[{sample}] MarkDup+Index: ✓ (exists)\")\n",
    "    else:\n",
    "        t2 = time.time()\n",
    "        if DEDUP_TOOL == \"classic\":\n",
    "            cmd = [\n",
    "                \"gatk\", \"MarkDuplicates\",\n",
    "                \"-I\", str(bam_sorted),\n",
    "                \"-O\", str(bam_dedup),\n",
    "                \"-M\", str(metrics),\n",
    "                \"--CREATE_INDEX\", \"true\",\n",
    "                \"--VALIDATION_STRINGENCY\", \"LENIENT\",\n",
    "                \"--verbosity\", \"ERROR\",\n",
    "            ]\n",
    "            rc, _, _ = run_silently(cmd, sample)\n",
    "            dt = timedelta(seconds=int(time.time() - t2))\n",
    "            if rc != 0 or not (bam_dedup.exists() and bai_dedup.exists()):\n",
    "                return (sample, False, f\"MarkDuplicates failed (see log: {lp.name})\")\n",
    "            print(f\"[{sample}] MarkDup+Index: ✓ in {dt}\")\n",
    "\n",
    "        elif DEDUP_TOOL == \"spark\":\n",
    "            tmp = os.environ.get(\"TMPDIR\", None) or os.environ.get(\"TMP\", None) or \"/tmp\"\n",
    "            cmd = [\n",
    "                \"gatk\", \"MarkDuplicatesSpark\",\n",
    "                \"-I\", str(bam_sorted),\n",
    "                \"-O\", str(bam_dedup),\n",
    "                \"--read-validation-stringency\", \"LENIENT\",\n",
    "                \"--spark-runner\", \"LOCAL\",\n",
    "                \"--spark-master\", f\"local[{DEDUP_THREADS}]\",\n",
    "                \"--conf\", f\"spark.local.dir={tmp}\",\n",
    "                \"--verbosity\", \"ERROR\",\n",
    "            ]\n",
    "            rc, _, _ = run_silently(cmd, sample)\n",
    "            # Spark sometimes does not auto-index; index silently\n",
    "            if rc == 0 and not (bai_dedup.exists()):\n",
    "                rc2, _, _ = run_silently([\"samtools\", \"index\", \"-@\", \"8\", str(bam_dedup)], sample)\n",
    "                if rc2 != 0:\n",
    "                    return (sample, False, f\"Index failed (see log: {lp.name})\")\n",
    "            dt = timedelta(seconds=int(time.time() - t2))\n",
    "            if rc == 0 and bam_dedup.exists() and bai_dedup.exists():\n",
    "                print(f\"[{sample}] MarkDup(+Index): ✓ in {dt}\")\n",
    "            else:\n",
    "                return (sample, False, f\"MarkDuplicatesSpark failed (see log: {lp.name})\")\n",
    "\n",
    "        elif DEDUP_TOOL == \"sambamba\":\n",
    "            cmd = [\"sambamba\", \"markdup\", \"--quiet\", \"-t\", str(DEDUP_THREADS), str(bam_sorted), str(bam_dedup)]\n",
    "            rc, _, _ = run_silently(cmd, sample)\n",
    "            if rc != 0 or not bam_dedup.exists():\n",
    "                return (sample, False, f\"sambamba markdup failed (see log: {lp.name})\")\n",
    "            # Index quietly\n",
    "            rc2, _, _ = run_silently([\"samtools\", \"index\", \"-@\", \"8\", str(bam_dedup)], sample)\n",
    "            if rc2 != 0:\n",
    "                return (sample, False, f\"Index failed (see log: {lp.name})\")\n",
    "            dt = timedelta(seconds=int(time.time() - t2))\n",
    "            print(f\"[{sample}] MarkDup(+Index): ✓ in {dt}\")\n",
    "\n",
    "    # 3) Cleanup\n",
    "    if bam_sorted.exists() and bam_dedup.exists() and bai_dedup.exists():\n",
    "        try:\n",
    "            bam_sorted.unlink()\n",
    "            print(f\"[{sample}] Cleanup: removed temp sorted BAM\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    dt_all = timedelta(seconds=int(time.time() - t0))\n",
    "    return (sample, True, f\"{dt_all}\")\n",
    "\n",
    "# ===== Worklist & concurrency =====\n",
    "jobs = []\n",
    "skipped = 0\n",
    "for sample in sorted(PAIRS.keys()):\n",
    "    bam_dedup  = ALN_DIR / f\"{sample}.dedup.bam\"\n",
    "    bai_dedup  = ALN_DIR / f\"{sample}.dedup.bam.bai\"\n",
    "    if (bam_dedup.exists() and bai_dedup.exists()):\n",
    "        skipped += 1\n",
    "    else:\n",
    "        jobs.append(sample)\n",
    "\n",
    "total = len(PAIRS)\n",
    "todo  = len(jobs)\n",
    "workers = _calc_workers(todo)\n",
    "\n",
    "print(f\"Align→Sort→Dedup for {total} sample(s) (to do={todo}, skipped={skipped})\")\n",
    "print(f\"Concurrency: workers={workers} | PER_SAMPLE_THREADS={PER_SAMPLE_THREADS} \"\n",
    "      f\"(BWA={BWA_THREADS}, SORT={SORT_THREADS}, DEDUP={DEDUP_THREADS}) | THREADS(total)={THREADS}\")\n",
    "\n",
    "if todo == 0:\n",
    "    print(\"Nothing to do. Outputs already present in:\", ALN_DIR)\n",
    "else:\n",
    "    start = time.time()\n",
    "    done = 0\n",
    "    failures = []\n",
    "\n",
    "    def eta_text(d, t, elapsed):\n",
    "        if d <= 0: return \"--:--:--\"\n",
    "        rate = d / max(elapsed, 1e-6)\n",
    "        rem = t - d\n",
    "        secs = int(rem / rate) if rate > 0 else 0\n",
    "        return str(timedelta(seconds=secs))\n",
    "\n",
    "    print(f\"[Progress] 0/{todo} | ETA=--:--:--\", end=\"\", flush=True)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        futmap = {ex.submit(process_one_sample, s): s for s in jobs}\n",
    "        for fut in concurrent.futures.as_completed(futmap):\n",
    "            sid = futmap[fut]\n",
    "            try:\n",
    "                sample, ok, msg = fut.result()\n",
    "                if not ok:\n",
    "                    failures.append((sid, msg))\n",
    "            except Exception as e:\n",
    "                failures.append((sid, str(e)))\n",
    "            finally:\n",
    "                done += 1\n",
    "                print(f\"\\r[Progress] {done}/{todo} | ETA={eta_text(done, todo, time.time()-start)}   \", end=\"\", flush=True)\n",
    "\n",
    "    print()  # newline\n",
    "    wrote = todo - len(failures)\n",
    "    print(\"\\nAlignment+Dedup complete.\")\n",
    "    print(\"Outputs:\", ALN_DIR)\n",
    "    print(f\"Summary: wrote={wrote}, skipped={skipped}, failed={len(failures)}, total={total}\")\n",
    "    if failures:\n",
    "        print(\"Failed samples (see per-sample logs in 'alignments/logs/'):\")\n",
    "        for sid, msg in failures:\n",
    "            print(f\"  - {sid}: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df81dc-8cb4-46b2-8b05-1c615df03979",
   "metadata": {},
   "source": [
    "## 6. Variant calling (VCF generation)\n",
    "### 6.1 Variant calling with FreeBayes (creates a cohort.freebayes.vcf.gz file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15756d73-60cc-4a96-ad9e-02344b13c48e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 443 scaffolds with up to 100 concurrent workers...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bbff6886ec47e3a811657407dfce1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntProgress(value=0, description='UCE call:', max=443), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## FreeBayes per-scaffold (robust pool, longest-first, with logs + progress bar)\n",
    "from pathlib import Path\n",
    "import subprocess, os, time, math\n",
    "from datetime import timedelta\n",
    "\n",
    "# Optional: nice Jupyter progress bar; will gracefully fallback if not present\n",
    "use_widgets = False\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    import ipywidgets as widgets\n",
    "    use_widgets = True\n",
    "except Exception:\n",
    "    use_widgets = False\n",
    "\n",
    "RUN_DIR   = Path(RUN_DIR)\n",
    "ALN_DIR   = RUN_DIR / \"alignments\"\n",
    "VAR_DIR   = RUN_DIR / \"variants\" / \"freebayes\"\n",
    "TMP_DIR   = VAR_DIR / \"_tmp_chunks\"\n",
    "for d in (VAR_DIR, TMP_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "REF_FASTA = Path(REF_FASTA)\n",
    "THREADS   = int(THREADS)\n",
    "\n",
    "# Gather BAMs\n",
    "bams = sorted(ALN_DIR.glob(\"*.dedup.bam\"))\n",
    "if not bams:\n",
    "    raise RuntimeError(\"No deduplicated BAMs found\")\n",
    "\n",
    "# Ensure indexes\n",
    "if not Path(str(REF_FASTA) + \".fai\").exists():\n",
    "    subprocess.run([\"samtools\", \"faidx\", str(REF_FASTA)], check=True)\n",
    "for b in bams:\n",
    "    if not Path(str(b) + \".bai\").exists():\n",
    "        subprocess.run([\"samtools\", \"index\", str(b)], check=True)\n",
    "\n",
    "# Read contigs + lengths, sort longest-first (reduces long-tail)\n",
    "contigs = []\n",
    "with open(str(REF_FASTA) + \".fai\") as f:\n",
    "    for line in f:\n",
    "        name, length, *_ = line.strip().split(\"\\t\")\n",
    "        contigs.append((name, int(length)))\n",
    "contigs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Helper to derive output chunk path\n",
    "def out_paths_for(ctg: str):\n",
    "    out_vcf = TMP_DIR / f\"{ctg}.vcf.gz\"\n",
    "    log     = TMP_DIR / f\"{ctg}.log\"\n",
    "    return out_vcf, log\n",
    "\n",
    "# Compute how many chunks are already done (idempotent re-runs)\n",
    "already_done = sum(1 for ctg, _L in contigs if out_paths_for(ctg)[0].exists())\n",
    "total_chunks = len(contigs) - already_done\n",
    "\n",
    "# Concurrency: safe for Jupyter; set to THREADS for full saturation\n",
    "jobs = max(1, THREADS // 2)\n",
    "print(f\"Running {len(contigs)} scaffolds with up to {jobs} concurrent workers...\")\n",
    "if already_done:\n",
    "    print(f\"Resuming: {already_done} scaffolds already completed from a previous run.\")\n",
    "\n",
    "def chunk_cmd(ctg):\n",
    "    out_vcf, log = out_paths_for(ctg)\n",
    "    # Skip if completed\n",
    "    if out_vcf.exists():\n",
    "        return None\n",
    "    cmd = (\n",
    "        f'freebayes -f \"{REF_FASTA}\" '\n",
    "        f'--region \"{ctg}\" --min-mapping-quality 20 --min-base-quality 20 --use-best-n-alleles 4 '\n",
    "        + \" \".join(f'\"{b}\"' for b in bams)\n",
    "        + f' 2> \"{log}\" | bgzip -c > \"{out_vcf}\"'\n",
    "    )\n",
    "    return cmd\n",
    "\n",
    "# Prepare queue of commands\n",
    "queue = [chunk_cmd(ctg) for ctg, _L in contigs]\n",
    "queue = [c for c in queue if c]  # drop None for already-done\n",
    "procs = {}  # pid -> (Popen, cmd)\n",
    "\n",
    "# Progress UI\n",
    "start_ts = time.time()\n",
    "completed_last = already_done\n",
    "\n",
    "if use_widgets:\n",
    "    pbar = widgets.IntProgress(\n",
    "      value=already_done, min=0, max=already_done + total_chunks, step=1, description='UCE call:', bar_style=''\n",
    "    )\n",
    "    status = widgets.HTML()\n",
    "    box = widgets.VBox([pbar, status])\n",
    "    display(box)\n",
    "else:\n",
    "    print(f\"[Progress] {already_done}/{already_done + total_chunks} completed | active=0 | queue={len(queue)} | ETA=--:--:--\")\n",
    "\n",
    "def eta_text(done, total, elapsed):\n",
    "    if done <= 0 or elapsed < 1e-6:\n",
    "        return \"--:--:--\"\n",
    "    rate = done / elapsed  # chunks per second\n",
    "    remain = max(0, total - done)\n",
    "    secs = int(remain / rate) if rate > 0 else 0\n",
    "    return str(timedelta(seconds=secs))\n",
    "\n",
    "def update_progress():\n",
    "    # Count completed by checking files (robust to any external completion)\n",
    "    done_now = sum(1 for ctg, _L in contigs if out_paths_for(ctg)[0].exists())\n",
    "    active = len(procs)\n",
    "    qrem   = len(queue)\n",
    "    elapsed = time.time() - start_ts\n",
    "    eta = eta_text(done_now - already_done, total_chunks, elapsed)\n",
    "    if use_widgets:\n",
    "        pbar.max = already_done + total_chunks\n",
    "        pbar.value = done_now\n",
    "        pbar.description = f\"{done_now}/{already_done + total_chunks}\"\n",
    "        status.value = f\"<code>active={active} | queue={qrem} | ETA={eta}</code>\"\n",
    "    else:\n",
    "        print(f\"\\r[Progress] {done_now}/{already_done + total_chunks} completed | active={active} | queue={qrem} | ETA={eta}    \", end='', flush=True)\n",
    "\n",
    "def launch():\n",
    "    while queue and len(procs) < jobs:\n",
    "        cmd = queue.pop(0)\n",
    "        p = subprocess.Popen(cmd, shell=True, executable=\"/bin/bash\")\n",
    "        procs[p.pid] = (p, cmd)\n",
    "\n",
    "# Prime the pool and show initial progress\n",
    "launch()\n",
    "update_progress()\n",
    "\n",
    "# Keep refilling until all work done\n",
    "while procs or queue:\n",
    "    time.sleep(0.5)  # slightly faster UI refresh\n",
    "    finished = []\n",
    "    for pid, (p, cmd) in list(procs.items()):\n",
    "        ret = p.poll()\n",
    "        if ret is not None:\n",
    "            finished.append(pid)\n",
    "            if ret != 0:\n",
    "                print(f\"\\n[WARN] Chunk exited {ret}: {cmd}\")\n",
    "    for pid in finished:\n",
    "        procs.pop(pid, None)\n",
    "    launch()\n",
    "    update_progress()\n",
    "\n",
    "# Ensure final progress line ends cleanly in text mode\n",
    "if not use_widgets:\n",
    "    print()\n",
    "\n",
    "# Concatenate all chunks (in fasta order)\n",
    "chunk_vcfs = [str(TMP_DIR / f\"{ctg}.vcf.gz\") for ctg, _L in contigs if (TMP_DIR / f\"{ctg}.vcf.gz\").exists()]\n",
    "if not chunk_vcfs:\n",
    "    raise RuntimeError(\"No chunk VCFs were produced; check logs in _tmp_chunks/\")\n",
    "\n",
    "vcf_gz = VAR_DIR / \"cohort.freebayes.vcf.gz\"\n",
    "subprocess.run([\"bcftools\", \"concat\", \"-Oz\", \"-o\", str(vcf_gz)] + chunk_vcfs, check=True)\n",
    "subprocess.run([\"bcftools\", \"index\", \"-t\", str(vcf_gz)], check=True)\n",
    "\n",
    "if use_widgets:\n",
    "    status.value = f\"<b>Done.</b> Output: <code>{vcf_gz}</code>\"\n",
    "else:\n",
    "    print(\"Done. Output:\", vcf_gz)\n",
    "print(\"Done. Output:\", vcf_gz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9faff64-190d-42b4-baf5-6a1d62dab1d9",
   "metadata": {},
   "source": [
    "### 6.2 Split cohort VCF into individual per-sample VCFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31e70f-b869-4c54-9414-043e0d1a1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split FreeBayes cohort VCF into per-sample VCFs (parallel + progress)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, concurrent.futures, os, time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Optional: Jupyter progress bar (fallback to text if widgets not available)\n",
    "use_widgets = False\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    import ipywidgets as widgets\n",
    "    use_widgets = True\n",
    "except Exception:\n",
    "    use_widgets = False\n",
    "\n",
    "# --- Paths / globals from earlier cells ---\n",
    "RUN_DIR  = Path(RUN_DIR)  # e.g., results/<read_set>\n",
    "VAR_DIR  = RUN_DIR / \"variants\" / \"freebayes\"\n",
    "COHORT_VCF = VAR_DIR / \"cohort.freebayes.vcf.gz\"\n",
    "PER_SAMPLE_DIR = VAR_DIR / \"per-sample\"\n",
    "PER_SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Sanity checks ---\n",
    "if not COHORT_VCF.exists():\n",
    "    raise RuntimeError(f\"Cohort VCF not found: {COHORT_VCF}. Run the FreeBayes calling cell first.\")\n",
    "\n",
    "tbi = Path(str(COHORT_VCF) + \".tbi\")\n",
    "if not tbi.exists():\n",
    "    subprocess.run([\"bcftools\", \"index\", \"-t\", str(COHORT_VCF)], check=True)\n",
    "\n",
    "# --- Get sample IDs from the cohort VCF header ---\n",
    "res = subprocess.run([\"bcftools\", \"query\", \"-l\", str(COHORT_VCF)],\n",
    "                     check=True, capture_output=True, text=True)\n",
    "samples_all = [s for s in res.stdout.strip().splitlines() if s]\n",
    "\n",
    "if not samples_all:\n",
    "    raise RuntimeError(\"No samples found in cohort VCF header.\")\n",
    "\n",
    "# --- Build worklist (skip samples that already have both files) ---\n",
    "work = []\n",
    "skipped = 0\n",
    "for s in samples_all:\n",
    "    out_vcf = PER_SAMPLE_DIR / f\"{s}.vcf.gz\"\n",
    "    out_tbi = Path(str(out_vcf) + \".tbi\")\n",
    "    if out_vcf.exists() and out_tbi.exists():\n",
    "        skipped += 1\n",
    "    else:\n",
    "        work.append((s, out_vcf))\n",
    "\n",
    "total = len(work)\n",
    "print(f\"Per-sample export: {len(samples_all)} sample(s) total \"\n",
    "      f\"(to do={total}, skipped={skipped})\")\n",
    "\n",
    "if total == 0:\n",
    "    print(\"Nothing to do.\")\n",
    "else:\n",
    "    # --- Parallelism (safe default): use ~1/4 of THREADS, at least 2, up to #samples ---\n",
    "    THREADS = int(THREADS)  # from your session\n",
    "    default_workers = max(2, THREADS // 4)\n",
    "    workers = min(default_workers, total)\n",
    "    # Allow override via a global if you want: JOBS_PER_SAMPLE_SPLIT\n",
    "    workers = int(globals().get(\"JOBS_PER_SAMPLE_SPLIT\", workers))\n",
    "    print(f\"Running {workers} parallel worker(s)...\")\n",
    "\n",
    "    # Progress UI\n",
    "    start_ts = time.time()\n",
    "    done = 0\n",
    "    if use_widgets:\n",
    "        pbar = widgets.IntProgress(value=0, min=0, max=total, description='Split:')\n",
    "        status = widgets.HTML()\n",
    "        box = widgets.VBox([pbar, status])\n",
    "        display(box)\n",
    "    else:\n",
    "        print(f\"[Progress] 0/{total} | ETA=--:--:--\", end='', flush=True)\n",
    "\n",
    "    def eta_text(done, total, elapsed):\n",
    "        if done <= 0 or elapsed <= 0:\n",
    "            return \"--:--:--\"\n",
    "        rate = done / elapsed  # samples per sec\n",
    "        remain = total - done\n",
    "        secs = int(remain / rate) if rate > 0 else 0\n",
    "        return str(timedelta(seconds=secs))\n",
    "\n",
    "    def update_progress():\n",
    "        #nonlocal done\n",
    "        elapsed = time.time() - start_ts\n",
    "        eta = eta_text(done, total, elapsed)\n",
    "        if use_widgets:\n",
    "            pbar.value = done\n",
    "            pbar.description = f\"{done}/{total}\"\n",
    "            status.value = f\"<code>workers={workers} | ETA={eta}</code>\"\n",
    "        else:\n",
    "            print(f\"\\r[Progress] {done}/{total} | ETA={eta}    \", end='', flush=True)\n",
    "\n",
    "    # Worker function\n",
    "    def extract_one(sample: str, out_vcf: Path):\n",
    "        # 1) bcftools view -s <sample> -Oz -o <out>\n",
    "        subprocess.run([\"bcftools\", \"view\", \"-s\", sample, \"-Oz\",\n",
    "                        \"-o\", str(out_vcf), str(COHORT_VCF)],\n",
    "                       check=True)\n",
    "        # 2) tabix index\n",
    "        subprocess.run([\"bcftools\", \"index\", \"-t\", str(out_vcf)], check=True)\n",
    "        return sample\n",
    "\n",
    "    # Launch pool\n",
    "    failures = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "        future_map = {ex.submit(extract_one, s, out): s for s, out in work}\n",
    "        for fut in concurrent.futures.as_completed(future_map):\n",
    "            s = future_map[fut]\n",
    "            try:\n",
    "                fut.result()\n",
    "            except Exception as e:\n",
    "                failures.append((s, str(e)))\n",
    "                print(f\"\\n[WARN] {s}: failed → {e}\")\n",
    "            finally:\n",
    "                done += 1\n",
    "                update_progress()\n",
    "\n",
    "    # Finalize progress line\n",
    "    if not use_widgets:\n",
    "        print()\n",
    "\n",
    "    # Summary\n",
    "    written = total - len(failures)\n",
    "    print(f\"\\nDone. Per-sample VCFs in: {PER_SAMPLE_DIR}\")\n",
    "    print(f\"Summary: written={written}, skipped={skipped}, failed={len(failures)}, total={len(samples_all)}\")\n",
    "    if failures:\n",
    "        print(\"Failed samples:\")\n",
    "        for s, msg in failures:\n",
    "            print(f\"  - {s}: {msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b617fb7-e555-4539-8e94-55ed6f8d69e7",
   "metadata": {},
   "source": [
    "## 7. Run Summary stats  \n",
    "### 7.1 Final summary cell  \n",
    "Note this was really written fro the previous complicated pipeline, that had some merit but was too complicated. Hence it is looking for alignemnet stats etc. Perhaps this is better now, and hopefully fast enough to run overnight or something for a spdier lab genome set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0141052-cc05-45ca-b511-fdb5654a1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final Summary Cell\n",
    "print(\"Started summary report cell...\")\n",
    "import os, re, json, shutil, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown  # <-- for Markdown rendering in Jupyter\n",
    "\n",
    "# ---------- Inputs from earlier cells ----------\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    raise RuntimeError(\"PROJECT_ROOT not set. Run the selection/setup cells first.\")\n",
    "\n",
    "READS_DIR = Path(READS_DIR) \n",
    "PAIRS         = globals().get(\"PAIRS\", {})\n",
    "REF_FASTA     = globals().get(\"REF_FASTA\", None)\n",
    "\n",
    "RUN_DIR       = Path(RUN_DIR)\n",
    "QC_RAW_DIR    = Path(QC_RAW_DIR)  \n",
    "#QC_TRIM   = Path(QC_TRIM)   # What does it want here\n",
    "TRIM_DIR  = Path(TRIM_DIR) \n",
    "ALN_DIR   = Path(ALN_DIR)\n",
    "VAR_DIR   = Path(VAR_DIR)\n",
    "LOGS_DIR  = RUN_DIR / \"logs\" \n",
    "REPORT_MD = RUN_DIR / \"run_report.md\"\n",
    "\n",
    "# Expected VCF (adjust name if you used a different filename)\n",
    "VCF_GZ   = VAR_DIR  / \"cohort.freebayes.vcf.gz\"\n",
    "VCF_TBI  = VAR_DIR  / \"cohort.freebayes.vcf.gz.tbi\"\n",
    "\n",
    "# ---------- Tool checks ----------\n",
    "def have_tool(name: str) -> bool:\n",
    "    return shutil.which(name) is not None\n",
    "\n",
    "if not have_tool(\"samtools\"):\n",
    "    raise RuntimeError(\"samtools not found in PATH.\")\n",
    "if not have_tool(\"bcftools\"):\n",
    "    raise RuntimeError(\"bcftools not found in PATH.\")\n",
    "\n",
    "def run_cmd(cmd: list[str]) -> str:\n",
    "    \"\"\"Run command and return stdout; raise if non-zero exit.\"\"\"\n",
    "    proc = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {' '.join(cmd)}\\n{proc.stderr}\")\n",
    "    return proc.stdout\n",
    "\n",
    "# ---------- Alignment stats per sample ----------\n",
    "align_stats = {}\n",
    "for sample_id in sorted(PAIRS.keys()):\n",
    "    bam_dedup = ALN_DIR / f\"{sample_id}.dedup.bam\"\n",
    "    md_metrics = ALN_DIR / f\"{sample_id}.markdup.metrics.txt\"\n",
    "    if not bam_dedup.exists():\n",
    "        print(f\"[WARN] Missing dedup BAM for sample {sample_id}: {bam_dedup}\")\n",
    "        continue\n",
    "    # samtools flagstat summary\n",
    "    fs = run_cmd([\"samtools\", \"flagstat\", str(bam_dedup)])\n",
    "    # Parse a few key values\n",
    "    total = mapped = prop = dups = None\n",
    "    for line in fs.splitlines():\n",
    "        if \" in total \" in line:\n",
    "            m = re.match(r\"(\\d+)\\s+\\+\\s+\\d+\\s+in total\", line)\n",
    "            if m: total = int(m.group(1))\n",
    "        elif \" mapped (\" in line:\n",
    "            m1 = re.match(r\"(\\d+)\\s+\\+\\s+\\d+\\s+mapped\\s+\\(([\\d\\.]+)%\", line)\n",
    "            if m1:\n",
    "                mapped = (int(m1.group(1)), float(m1.group(2)))\n",
    "        elif \" properly paired \" in line:\n",
    "            m2 = re.match(r\"(\\d+)\\s+\\+\\s+\\d+\\s+properly paired\\s+\\(([\\d\\.]+)%\", line)\n",
    "            if m2:\n",
    "                prop = (int(m2.group(1)), float(m2.group(2)))\n",
    "        elif \" duplicates\" in line:\n",
    "            m3 = re.match(r\"(\\d+)\\s+\\+\\s+\\d+\\s+duplicates\", line)\n",
    "            if m3: dups = int(m3.group(1))\n",
    "\n",
    "    # Parse duplication rate from MarkDuplicates metrics (Picard/GATK)\n",
    "    dup_rate = None\n",
    "    if md_metrics.exists():\n",
    "        txt = Path(md_metrics).read_text(errors=\"ignore\").splitlines()\n",
    "        header_idx = None\n",
    "        for i, ln in enumerate(txt):\n",
    "            if ln.strip().startswith(\"LIBRARY\\t\"):\n",
    "                header_idx = i\n",
    "                break\n",
    "        if header_idx is not None and header_idx + 1 < len(txt):\n",
    "            header = txt[header_idx].split(\"\\t\")\n",
    "            data = txt[header_idx+1].split(\"\\t\")\n",
    "            if \"PERCENT_DUPLICATION\" in header and len(data) == len(header):\n",
    "                j = header.index(\"PERCENT_DUPLICATION\")\n",
    "                try:\n",
    "                    dup_rate = float(data[j])\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    align_stats[sample_id] = {\n",
    "        \"total_reads\": total,\n",
    "        \"mapped_reads\": mapped[0] if mapped else None,\n",
    "        \"mapped_pct\": mapped[1] if mapped else None,\n",
    "        \"proper_pairs\": prop[0] if prop else None,\n",
    "        \"proper_pairs_pct\": prop[1] if prop else None,\n",
    "        \"dup_reads\": dups,\n",
    "        \"dup_rate_metric\": dup_rate,  # from MarkDuplicates (0..1)\n",
    "        \"bam\": str(bam_dedup),\n",
    "    }\n",
    "\n",
    "# ---------- VCF-level stats ----------\n",
    "vcf_stats = {\n",
    "    \"vcf_path\": str(VCF_GZ),\n",
    "    \"indexed\": VCF_TBI.exists(),\n",
    "    \"samples\": [],\n",
    "    \"n_variants\": None,\n",
    "    \"n_snps\": None,\n",
    "    \"n_indels\": None,\n",
    "    \"ts_tv\": None\n",
    "}\n",
    "\n",
    "if VCF_GZ.exists():\n",
    "    # Samples\n",
    "    vcf_stats[\"samples\"] = run_cmd([\"bcftools\", \"query\", \"-l\", str(VCF_GZ)]).split()\n",
    "    # bcftools stats\n",
    "    stats_txt = run_cmd([\"bcftools\", \"stats\", str(VCF_GZ)])\n",
    "    # Parse SN and TSTV lines\n",
    "    for line in stats_txt.splitlines():\n",
    "        if line.startswith(\"SN\"):\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 4:\n",
    "                label = parts[2].strip().lower()\n",
    "                val = parts[3].strip()\n",
    "                if label == \"number of records:\":\n",
    "                    vcf_stats[\"n_variants\"] = int(val)\n",
    "                elif label == \"number of snps:\":\n",
    "                    vcf_stats[\"n_snps\"] = int(val)\n",
    "                elif label == \"number of indels:\":\n",
    "                    vcf_stats[\"n_indels\"] = int(val)\n",
    "        elif line.startswith(\"TSTV\"):\n",
    "            parts = line.split(\"\\t\")\n",
    "            # Expect: TSTV  0  ALL  nTransitions  nTransversions  ts/tv\n",
    "            if len(parts) >= 6 and (parts[2].upper() == \"ALL\"):\n",
    "                try:\n",
    "                    vcf_stats[\"ts_tv\"] = float(parts[5])\n",
    "                except Exception:\n",
    "                    pass\n",
    "else:\n",
    "    print(f\"[WARN] VCF not found: {VCF_GZ}\")\n",
    "\n",
    "# ---------- Disk usage info ----------\n",
    "def human_bytes(n):\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]\n",
    "    i = 0\n",
    "    f = float(n)\n",
    "    while f >= 1024 and i < len(units)-1:\n",
    "        f /= 1024; i += 1\n",
    "    return f\"{f:.2f} {units[i]}\"\n",
    "\n",
    "def total_size(path: Path) -> int:\n",
    "    total = 0\n",
    "    if not path.exists(): return 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            p = Path(root) / f\n",
    "            try:\n",
    "                total += p.stat().st_size\n",
    "            except Exception:\n",
    "                pass\n",
    "    return total\n",
    "\n",
    "disk = shutil.disk_usage(str(RUN_DIR))\n",
    "sizes = {\n",
    "    \"trimmed\": total_size(TRIM_DIR),\n",
    "    \"alignments\": total_size(ALN_DIR),\n",
    "    \"variants\": total_size(VAR_DIR),\n",
    "    \"qc_raw\": total_size(QC_RAW_DIR),\n",
    "    # \"qc_trim\": total_size(QC_TRIM),\n",
    "}\n",
    "\n",
    "# ---------- Build Markdown report ----------\n",
    "lines = []\n",
    "lines.append(f\"# Run Report — {READ_SET_NAME}\\n\")\n",
    "lines.append(f\"_Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_\\n\")\n",
    "lines.append(\"## Paths\")\n",
    "lines.append(f\"- Project root: `{PROJECT_ROOT}`\")\n",
    "lines.append(f\"- Run dir: `{RUN_DIR}`\")\n",
    "if REF_FASTA: lines.append(f\"- Reference (run-local): `{REF_FASTA}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Input summary\")\n",
    "lines.append(f\"- Samples detected: **{len(PAIRS)}**\")\n",
    "lines.append(f\"- Trimmed reads dir: `{TRIM_DIR}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Alignment summary (per sample)\")\n",
    "if align_stats:\n",
    "    lines.append(\"| Sample | Total reads | Mapped (%) | Proper pairs (%) | Duplicates | Dup. rate (MarkDup) | BAM |\")\n",
    "    lines.append(\"|---|---:|---:|---:|---:|---:|---|\")\n",
    "    for sid, st in align_stats.items():\n",
    "        m_pct = f\"{st['mapped_pct']:.2f}%\" if st['mapped_pct'] is not None else \"NA\"\n",
    "        p_pct = f\"{st['proper_pairs_pct']:.2f}%\" if st['proper_pairs_pct'] is not None else \"NA\"\n",
    "        dup_reads = f\"{st['dup_reads']:,}\" if st['dup_reads'] is not None else \"NA\"\n",
    "        dup_rate = f\"{st['dup_rate_metric']*100:.2f}%\" if st['dup_rate_metric'] is not None else \"NA\"\n",
    "        lines.append(f\"| {sid} | {st['total_reads'] or 'NA'} | {m_pct} | {p_pct} | {dup_reads} | {dup_rate} | `{st['bam']}` |\")\n",
    "else:\n",
    "    lines.append(\"_No alignment stats found. Did the alignment cell run?_\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## VCF summary\")\n",
    "if VCF_GZ.exists():\n",
    "    lines.append(f\"- VCF: `{VCF_GZ.name}` (indexed: **{'yes' if vcf_stats['indexed'] else 'no'}**)\")\n",
    "    lines.append(f\"- Samples in VCF: **{len(vcf_stats['samples'])}**\")\n",
    "    lines.append(f\"- Total variants: **{vcf_stats['n_variants'] if vcf_stats['n_variants'] is not None else 'NA'}**\")\n",
    "    lines.append(f\"- SNPs: **{vcf_stats['n_snps'] if vcf_stats['n_snps'] is not None else 'NA'}**\")\n",
    "    lines.append(f\"- Indels: **{vcf_stats['n_indels'] if vcf_stats['n_indels'] is not None else 'NA'}**\")\n",
    "    lines.append(f\"- Ts/Tv: **{vcf_stats['ts_tv'] if vcf_stats['ts_tv'] is not None else 'NA'}**\")\n",
    "else:\n",
    "    lines.append(f\"- VCF not found at `{VCF_GZ}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Disk usage for this run\")\n",
    "lines.append(f\"- Run dir free/total: {human_bytes(disk.free)} / {human_bytes(disk.total)}\")\n",
    "for k, v in sizes.items():\n",
    "    lines.append(f\"- {k}: {human_bytes(v)}\")\n",
    "lines.append(\"\")\n",
    "\n",
    "# Write & render report\n",
    "REPORT_MD.write_text(\"\\n\".join(lines))\n",
    "display(Markdown(REPORT_MD.read_text()))  # <-- render as Markdown in the notebook\n",
    "\n",
    "# Also produce a machine-readable manifest (optional)\n",
    "manifest = {\n",
    "    \"read_set\": READ_SET_NAME,\n",
    "    \"run_dir\": str(RUN_DIR),\n",
    "    \"reference\": str(REF_FASTA) if REF_FASTA else None,\n",
    "    \"samples\": sorted(PAIRS.keys()),\n",
    "    \"alignment\": align_stats,\n",
    "    \"vcf\": vcf_stats,\n",
    "    \"sizes\": {k: int(v) for k, v in sizes.items()},\n",
    "    \"generated_at\": datetime.now().isoformat()\n",
    "}\n",
    "with open(RUN_DIR / \"run_manifest.json\", \"w\") as jf:\n",
    "    json.dump(manifest, jf, indent=2)\n",
    "\n",
    "# You can also show the \"Saved\" message as Markdown if you prefer:\n",
    "display(Markdown(\n",
    "    f\"**Saved:**  \\n- `{REPORT_MD}`  \\n- `{RUN_DIR / 'run_manifest.json'}`\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45d8e2-d360-4147-b81a-24752ba1cf2b",
   "metadata": {},
   "source": [
    "### 7.2 VCF QC Plots, - I don't know what they mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15766e9-5674-4024-b1a8-ab14f7b0c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Quick VCF QC plots: Ti/Tv overall + by contig, DP and QUAL histograms\n",
    "import subprocess, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Resolve paths & pick VCF ---\n",
    "VAR_DIR = Path(VAR_DIR)\n",
    "vcf_candidates = [\n",
    "    VAR_DIR / \"cohort.freebayes.vcf.gz\",\n",
    "]\n",
    "vcf = next((v for v in vcf_candidates if v.exists()), None)\n",
    "if vcf is None:\n",
    "    raise RuntimeError(\"No cohort VCF found. Run the variant calling pipeline first.\")\n",
    "\n",
    "# --- Tool sanity check ---\n",
    "if shutil.which(\"bcftools\") is None:\n",
    "    raise RuntimeError(\"bcftools not in PATH; required for bcftools stats/query.\")\n",
    "\n",
    "def run_bcftools_stats_any(vcf_path: Path, ref_fa, out_path: Path):\n",
    "    \"\"\"\n",
    "    Robust bcftools stats:\n",
    "      A) global -o before subcommand (preferred for some builds)\n",
    "      B) capture stdout (works everywhere)\n",
    "    Falls back cleanly; writes out_path on success.\n",
    "    \"\"\"\n",
    "    # Prefer with reference if provided\n",
    "    with_ref = (ref_fa is not None) and str(ref_fa).strip()\n",
    "\n",
    "    # Try A: global -o\n",
    "    try:\n",
    "        if with_ref:\n",
    "            cmd = [\"bcftools\", \"-o\", str(out_path), \"stats\", \"-F\", str(ref_fa), \"-s\", \"-\", str(vcf_path)]\n",
    "        else:\n",
    "            cmd = [\"bcftools\", \"-o\", str(out_path), \"stats\", \"-s\", \"-\", str(vcf_path)]\n",
    "        r = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if r.returncode == 0 and out_path.exists() and out_path.stat().st_size > 0:\n",
    "            return\n",
    "    except Exception:\n",
    "        pass  # fall through\n",
    "\n",
    "    # Try B: capture stdout (with -F if available)\n",
    "    if with_ref:\n",
    "        r = subprocess.run([\"bcftools\", \"stats\", \"-F\", str(ref_fa), \"-s\", \"-\", str(vcf_path)],\n",
    "                           capture_output=True, text=True)\n",
    "        if r.returncode == 0 and r.stdout:\n",
    "            out_path.write_text(r.stdout)\n",
    "            return\n",
    "\n",
    "    # Final fallback: capture stdout without -F\n",
    "    r = subprocess.run([\"bcftools\", \"stats\", \"-s\", \"-\", str(vcf_path)],\n",
    "                       capture_output=True, text=True)\n",
    "    if r.returncode == 0 and r.stdout:\n",
    "        out_path.write_text(r.stdout)\n",
    "        return\n",
    "\n",
    "    raise RuntimeError(f\"bcftools stats failed.\\nSTDERR:\\n{r.stderr}\")\n",
    "\n",
    "# --- bcftools stats (cache) ---\n",
    "stats_path = vcf.with_suffix(\".stats.txt\")\n",
    "if not stats_path.exists() or stats_path.stat().st_mtime < vcf.stat().st_mtime:\n",
    "    print(\"Running bcftools stats...\")\n",
    "    run_bcftools_stats_any(vcf, REF_FASTA, stats_path)\n",
    "\n",
    "# --- Parse bcftools stats: SN (summary) and TSTV (per-contig) ---\n",
    "sn_rows, tstv_rows = [], []\n",
    "with open(stats_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.startswith(\"SN\"):\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) >= 4:\n",
    "                key = parts[2]\n",
    "                val_str = parts[3]\n",
    "                try:\n",
    "                    val = float(val_str)\n",
    "                except ValueError:\n",
    "                    val = val_str\n",
    "                sn_rows.append((key, val))\n",
    "        elif line.startswith(\"TSTV\"):\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) >= 6:\n",
    "                chrom = parts[2]\n",
    "                try:\n",
    "                    ts = float(parts[3])\n",
    "                except ValueError:\n",
    "                    ts = np.nan\n",
    "                try:\n",
    "                    tv = float(parts[4])\n",
    "                except ValueError:\n",
    "                    tv = np.nan\n",
    "                try:\n",
    "                    ratio = float(parts[5]) if parts[5] not in (\"nan\", \".\", \"NA\") else np.nan\n",
    "                except ValueError:\n",
    "                    ratio = np.nan\n",
    "                tstv_rows.append((chrom, ts, tv, ratio))\n",
    "\n",
    "sn_df = pd.DataFrame(sn_rows, columns=[\"metric\", \"value\"]) if sn_rows else pd.DataFrame(columns=[\"metric\", \"value\"])\n",
    "tstv_df = pd.DataFrame(tstv_rows, columns=[\"chrom\", \"ts\", \"tv\", \"tstv\"]) if tstv_rows else pd.DataFrame(columns=[\"chrom\", \"ts\", \"tv\", \"tstv\"])\n",
    "\n",
    "# --- Key summary metrics ---\n",
    "overall_tstv = None\n",
    "n_snp = None\n",
    "if not sn_df.empty:\n",
    "    if (sn_df[\"metric\"] == \"TSTV\").any():\n",
    "        try:\n",
    "            overall_tstv = float(sn_df.loc[sn_df[\"metric\"] == \"TSTV\", \"value\"].iloc[0])\n",
    "        except Exception:\n",
    "            overall_tstv = None\n",
    "    if (sn_df[\"metric\"] == \"number of SNPs\").any():\n",
    "        try:\n",
    "            n_snp = int(float(sn_df.loc[sn_df[\"metric\"] == \"number of SNPs\", \"value\"].iloc[0]))\n",
    "        except Exception:\n",
    "            n_snp = None\n",
    "\n",
    "# --- INFO/DP and QUAL via bcftools query ---\n",
    "def _capture_lines(cmd):\n",
    "    r = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    return [ln.strip() for ln in r.stdout.splitlines() if ln.strip()]\n",
    "\n",
    "dp_lines  = _capture_lines([\"bcftools\", \"query\", \"-f\", \"%INFO/DP\\n\", str(vcf)])\n",
    "qual_lines = _capture_lines([\"bcftools\", \"query\", \"-f\", \"%QUAL\\n\", str(vcf)])\n",
    "\n",
    "def _to_numeric_np(lines):\n",
    "    vals = []\n",
    "    for s in lines:\n",
    "        if s in (\".\", \"NA\", \"NaN\", \"nan\", \"\"):\n",
    "            continue\n",
    "        try:\n",
    "            vals.append(float(s))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return np.array(vals, dtype=float)\n",
    "\n",
    "dp_vals   = _to_numeric_np(dp_lines)\n",
    "qual_vals = _to_numeric_np(qual_lines)\n",
    "\n",
    "# --- Plots ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# (1) Overall panel\n",
    "plt.subplot(1, 3, 1)\n",
    "if overall_tstv is not None:\n",
    "    plt.title(f\"Overall Ti/Tv = {overall_tstv:.2f}\\nSNPs={n_snp if n_snp is not None else 'NA'}\")\n",
    "else:\n",
    "    plt.title(\"Overall Ti/Tv (NA)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# (2) Per-contig Ti/Tv (top 20)\n",
    "plt.subplot(1, 3, 2)\n",
    "if not tstv_df.empty and tstv_df[\"tstv\"].notna().any():\n",
    "    tstv_plot = tstv_df.dropna(subset=[\"tstv\"]).sort_values(\"tstv\", ascending=False).head(20)\n",
    "    if not tstv_plot.empty:\n",
    "        plt.barh(tstv_plot[\"chrom\"], tstv_plot[\"tstv\"], color=\"#4e79a7\")\n",
    "        plt.xlabel(\"Ti/Tv by contig (top 20)\")\n",
    "        plt.gca().invert_yaxis()\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No numeric per-contig Ti/Tv\", ha=\"center\")\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No per-contig Ti/Tv\", ha=\"center\")\n",
    "\n",
    "# (3) Depth histogram\n",
    "plt.subplot(1, 3, 3)\n",
    "if dp_vals.size > 0:\n",
    "    vmax = np.nanpercentile(dp_vals, 99)\n",
    "    plt.hist(dp_vals[dp_vals <= vmax], bins=50, color=\"#59a14f\")\n",
    "    plt.xlabel(\"INFO/DP (site depth)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Depth distribution (99th pct clipped)\")\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No DP values\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# QUAL histogram\n",
    "plt.figure(figsize=(5, 4))\n",
    "if qual_vals.size > 0:\n",
    "    qmax = np.nanpercentile(qual_vals, 99)\n",
    "    plt.hist(qual_vals[qual_vals <= qmax], bins=50, color=\"#e15759\")\n",
    "    plt.xlabel(\"QUAL\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"QUAL distribution (99th pct clipped)\")\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"No QUAL values\", ha=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tip: On chromosome-scale assemblies, evaluate Ti/Tv per chromosome and compare autosomes vs. known/putative sex chromosomes. Low Ti/Tv in specific regions may indicate repeats or low mappability.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddd0cd-43d2-4938-a858-5d8a8db9456d",
   "metadata": {},
   "source": [
    "## 8 End of Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607c34f-a59a-415d-ad0f-7c7d4d6c2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚧 Stop here unless explicitly allowed\n",
    "import os, sys\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "# 🎉 **Pipeline completed sucessfully!**\n",
    "All analysis steps completed **successfully**.\n",
    "system exit \n",
    "\"\"\"))\n",
    "print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "\n",
    "raise SystemExit(0)  # clean stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61caf5c2-3260-496f-a950-75e397bf851b",
   "metadata": {},
   "source": [
    "## 9. Endnotes: Odd/cool cells from the writing process put here for storage - _these are not the pipeline_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5614b16-9805-4a80-a9d1-2e5f1fec8faa",
   "metadata": {},
   "source": [
    "## SNAKEMAKE launcher cell - Keep for later improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2b7a8-4b9c-4cb2-8f70-7619f279f351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Snakemake launcher cell ============================================\n",
    "# Run the Snakemake workflow from inside the Jupyter notebook.\n",
    "#\n",
    "# Requirements:\n",
    "#  - workflow/Snakefile exists (we'll build it next)\n",
    "#  - profiles/slurm/ contains Snakemake profile files (optional)\n",
    "#  - PAIRS, READ_SET_NAME, REF_FASTA exist (from earlier cells)\n",
    "#\n",
    "# Modes:\n",
    "#   mode = \"local\"  → single-node run (good for tiny test dataset)\n",
    "#   mode = \"slurm\"  → submit jobs to Slurm using your profile\n",
    "#\n",
    "# =======================================================================\n",
    "\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Select mode ---\n",
    "mode = \"local\"        # <-- change to \"slurm\" when ready\n",
    "#mode = \"slurm\"\n",
    "\n",
    "# --- Safety checks ---\n",
    "if \"PAIRS\" not in globals() or len(PAIRS) == 0:\n",
    "    raise RuntimeError(\"PAIRS dict is empty. Run your read‑selection cell first.\")\n",
    "\n",
    "if \"REF_FASTA\" not in globals():\n",
    "    raise RuntimeError(\"REF_FASTA not defined. Run reference‑indexing cell first.\")\n",
    "\n",
    "# --- Directory setup ---\n",
    "PROJECT_ROOT = Path(PROJECT_ROOT)\n",
    "print(\"project_root = \", PROJECT_ROOT)\n",
    "READ_SET_NAME = READ_SET_NAME if \"READ_SET_NAME\" in globals() else read_dir_dropdown.value\n",
    "RUN_DIR = PROJECT_ROOT / \"results\" / READ_SET_NAME\n",
    "CFG_DIR = PROJECT_ROOT / \"workflow\" / \"config\"\n",
    "CFG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SNAKEFILE = PROJECT_ROOT / \"Snakefile\"\n",
    "\n",
    "# --- Write samples.tsv from PAIRS ---\n",
    "samples_tsv = CFG_DIR / \"samples.tsv\"\n",
    "with open(samples_tsv, \"w\") as f:\n",
    "    f.write(\"sample\\tR1\\tR2\\n\")\n",
    "    for sid, p in sorted(PAIRS.items()):\n",
    "        f.write(f\"{sid}\\t{p['R1']}\\t{p['R2']}\\n\")\n",
    "\n",
    "# --- Write config.yaml ---\n",
    "config_yaml = CFG_DIR / \"config.yaml\"\n",
    "config_yaml.write_text(\n",
    "    f\"\"\"\n",
    "project_root: {PROJECT_ROOT}\n",
    "read_set: {READ_SET_NAME}\n",
    "reference: {Path(REF_FASTA).name}      # basename only; Snakefile resolves path\n",
    "caller: bcftools                       # or \"gatk\"\n",
    "threads_per_sample: 16\n",
    "mem_gb_per_sample: 64\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"### 📝 Wrote config files:\\n- `{samples_tsv}`\\n- `{config_yaml}`\"))\n",
    "\n",
    "# --- Snakemake command ---\n",
    "snk = [\n",
    "    \"snakemake\",\n",
    "    \"--directory\", str(PROJECT_ROOT),\n",
    "    \"-s\", str(SNAKEFILE),\n",
    "    \"--cores\", \"1\",                         # local scheduler cores\n",
    "    \"--printshellcmds\",\n",
    "    #\"--rerun-incomplete\",\n",
    "    \"--forceall\",                          # for debug\n",
    "]\n",
    "\n",
    "if mode == \"slurm\":\n",
    "    snk += [\n",
    "        \"--profile\", \"/group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/profiles/slurm\",\n",
    "        \"--jobs\", \"80\",\n",
    "    ]\n",
    "else:\n",
    "    # local mode for tiny tests\n",
    "    snk += [\"--cores\", str(CPU_THREADS_SUGGESTED)]\n",
    "\n",
    "display(Markdown(f\"### 🚀 Running Snakemake in **{mode}** mode...\\n```\\n{' '.join(snk)}\\n```\"))\n",
    "\n",
    "# --- Run Snakemake and stream output ---\n",
    "process = subprocess.Popen(snk, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "for line in process.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "process.wait()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    display(Markdown(\"### 🎉 Snakemake pipeline complete!\"))\n",
    "else:\n",
    "    raise RuntimeError(f\"Snakemake failed with exit code {process.returncode}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeef24d-b657-4d24-a5f6-9b603364fef4",
   "metadata": {},
   "source": [
    "#### Make tiny test dataset cell - keep in case needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e09b4-2c8e-4346-ad79-cbce3fec2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests/make_tiny_dataset.py\n",
    "#\n",
    "# Generates a tiny reference + two paired-end samples.\n",
    "# - Sample1 matches reference\n",
    "# - Sample2 has a few SNPs on ctgA\n",
    "# - A fraction of reads include real adapter tails (i7 on R1 3', i5 on R2 3')\n",
    "# - Outputs gzipped FASTQs\n",
    "#\n",
    "# Uses real adapters from:\n",
    "#   /group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline/data/adapters/adapters.fasta\n",
    "#\n",
    "# After running, you can symlink/copy:\n",
    "#   tests/tiny/ref/tiny.fa        -> data/references/tiny.fa\n",
    "#   tests/tiny/reads/*.fastq.gz   -> data/read_directories/tiny_test/\n",
    "#\n",
    "from pathlib import Path\n",
    "import random\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "PROJECT_ROOT = Path(\"/group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline\")\n",
    "\n",
    "ADAPTERS_REAL = PROJECT_ROOT / \"data\" / \"adapters\" / \"adapters.fasta\"\n",
    "OUT_ROOT      = Path(\"tests\") / \"tiny\"\n",
    "REF_DIR       = OUT_ROOT / \"ref\"\n",
    "READS_DIR     = OUT_ROOT / \"reads\"\n",
    "ADAPT_DIR     = OUT_ROOT / \"adapters\"    # optional local reference to adapters for the tiny test\n",
    "\n",
    "REF_NAME      = \"tiny.fa\"\n",
    "SAMPLES       = [\"sample1\", \"sample2\"]\n",
    "\n",
    "READ_LEN      = 100     # length per end\n",
    "INSERT_MEAN   = 300     # typical insert length\n",
    "N_PAIRS       = 3000    # pairs per sample; keep small to run fast\n",
    "ADAPT_FRAC    = 0.30    # fraction of pairs that will include adapter tails\n",
    "\n",
    "RNG_SEED      = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def rand_dna(n):\n",
    "    return \"\".join(random.choice(\"ACGT\") for _ in range(n))\n",
    "\n",
    "def rc(seq):\n",
    "    comp = str.maketrans(\"ACGT\", \"TGCA\")\n",
    "    return seq.translate(comp)[::-1]\n",
    "\n",
    "def load_adapters(fa_path):\n",
    "    \"\"\"\n",
    "    Parse a simple FASTA file containing:\n",
    "      >i5\n",
    "      <seq>\n",
    "      >i7\n",
    "      <seq>\n",
    "    Returns dict {'i5': seq, 'i7': seq}\n",
    "    \"\"\"\n",
    "    if not fa_path.exists():\n",
    "        raise FileNotFoundError(f\"Adapters FASTA not found at: {fa_path}\")\n",
    "\n",
    "    adapters = {}\n",
    "    name = None\n",
    "    seqs = []\n",
    "    with open(fa_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if name and seqs:\n",
    "                    adapters[name] = \"\".join(seqs).upper()\n",
    "                name = line[1:].strip().split()[0]  # take token after '>'\n",
    "                seqs = []\n",
    "            else:\n",
    "                seqs.append(line.strip())\n",
    "        if name and seqs:\n",
    "            adapters[name] = \"\".join(seqs).upper()\n",
    "\n",
    "    # Normalize keys: we expect 'i5' and 'i7'\n",
    "    norm = {}\n",
    "    for k, v in adapters.items():\n",
    "        key = k.lower()\n",
    "        if key in (\"i5\", \"i7\"):\n",
    "            norm[key] = re.sub(r\"[^ACGT]\", \"\", v.upper())\n",
    "    if \"i5\" not in norm or \"i7\" not in norm:\n",
    "        raise ValueError(f\"Adapters file must contain >i5 and >i7 entries. Found keys: {list(norm.keys())}\")\n",
    "    return norm\n",
    "\n",
    "def write_fastq_gz(path, records):\n",
    "    with gzip.open(path, \"wt\") as f:\n",
    "        for name, seq, qual in records:\n",
    "            f.write(f\"@{name}\\n{seq}\\n+\\n{qual}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    random.seed(RNG_SEED)\n",
    "    REF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    READS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ADAPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load real adapters\n",
    "    adapters = load_adapters(ADAPTERS_REAL)\n",
    "    i5 = adapters[\"i5\"]\n",
    "    i7 = adapters[\"i7\"]\n",
    "\n",
    "    # Symlink/copy adapters into tiny test dir (optional convenience)\n",
    "    target_adapt = ADAPT_DIR / \"adapters.fasta\"\n",
    "    if not target_adapt.exists():\n",
    "        try:\n",
    "            target_adapt.symlink_to(ADAPTERS_REAL)\n",
    "        except Exception:\n",
    "            # Fallback: copy\n",
    "            target_adapt.write_text(ADAPTERS_REAL.read_text())\n",
    "\n",
    "    # Build a tiny reference\n",
    "    contigs = {\n",
    "        \"ctgA\": list(rand_dna(30000)),\n",
    "        \"ctgB\": list(rand_dna(20000)),\n",
    "    }\n",
    "\n",
    "    # SNP sites that sample2 will carry as ALT (ref stays as-is)\n",
    "    snp_sites = [1000, 5000, 12000, 18000, 25000]  # on ctgA\n",
    "    # For sample2 we’ll flip bases at these positions (simulate true polymorphisms)\n",
    "    alt_bases = {}\n",
    "    for pos in snp_sites:\n",
    "        refb = contigs[\"ctgA\"][pos]\n",
    "        alts = [b for b in \"ACGT\" if b != refb]\n",
    "        altb = random.choice(alts)\n",
    "        alt_bases[pos] = altb\n",
    "\n",
    "    ref_path = REF_DIR / REF_NAME\n",
    "    with open(ref_path, \"w\") as f:\n",
    "        for name, seq_list in contigs.items():\n",
    "            f.write(f\">{name}\\n\")\n",
    "            seq = \"\".join(seq_list)\n",
    "            for i in range(0, len(seq), 60):\n",
    "                f.write(seq[i:i+60] + \"\\n\")\n",
    "\n",
    "    print(f\"[ok] Wrote reference: {ref_path}\")\n",
    "\n",
    "    # Build per-sample sequence dicts (strings)\n",
    "    seqs_s1 = {k: \"\".join(v) for k, v in contigs.items()}\n",
    "    # Sample2: apply ALT at snp_sites on ctgA (only to sample2 basis)\n",
    "    s2_ctgA = list(seqs_s1[\"ctgA\"])\n",
    "    for pos, alt in alt_bases.items():\n",
    "        s2_ctgA[pos] = alt\n",
    "    seqs_s2 = dict(seqs_s1)\n",
    "    seqs_s2[\"ctgA\"] = \"\".join(s2_ctgA)\n",
    "\n",
    "    # Generate paired reads with a fraction containing adapter tails.\n",
    "    # Model: For ADAPT_FRAC of fragments, create an insert shorter than READ_LEN,\n",
    "    # so R1 and R2 have 3' overhangs filled by adapters (R1 gets i7 tail; R2 gets i5 tail).\n",
    "    def generate_pairs(sample, seqs, n_pairs=N_PAIRS, read_len=READ_LEN, insert_mean=INSERT_MEAN, adapt_frac=ADAPT_FRAC):\n",
    "        qual = \"I\" * read_len  # simple high-quality score\n",
    "        r1_records = []\n",
    "        r2_records = []\n",
    "\n",
    "        keys = list(seqs.keys())\n",
    "        for idx in range(n_pairs):\n",
    "            ctg = random.choice(keys)\n",
    "            seq = seqs[ctg]\n",
    "\n",
    "            # Choose insert length\n",
    "            if random.random() < adapt_frac:\n",
    "                # short fragment (force adapter tails)\n",
    "                frag_len = random.randint(30, read_len - 10)  # 20..90-ish\n",
    "            else:\n",
    "                # near mean with some variance\n",
    "                frag_len = max(read_len + 20, int(random.gauss(insert_mean, insert_mean * 0.1)))\n",
    "\n",
    "            if len(seq) <= frag_len + 10:\n",
    "                # pick another contig if tiny; if still small, clamp\n",
    "                frag_len = min(frag_len, len(seq) - 10)\n",
    "\n",
    "            start = random.randint(0, max(0, len(seq) - frag_len - 1))\n",
    "            frag = seq[start:start+frag_len]\n",
    "\n",
    "            # R1 = forward 5'->3' from frag start\n",
    "            r1seq = frag[:read_len]\n",
    "            # R2 = reverse complement from frag end\n",
    "            r2seq = rc(frag[-read_len:])\n",
    "\n",
    "            # If fragment is shorter than read length, append adapter tails on 3'\n",
    "            if len(frag) < read_len:\n",
    "                overhang = read_len - len(frag)\n",
    "                # R1 3' tail with i7\n",
    "                r1_tail = (i7 * ((overhang // len(i7)) + 1))[:overhang]\n",
    "                r1seq = frag + r1_tail\n",
    "\n",
    "                # R2 3' tail with i5 (remember r2seq is RC of end of frag; appending tail in sequence-space)\n",
    "                r2_tail = (i5 * ((overhang // len(i5)) + 1))[:overhang]\n",
    "                # Because r2seq is already reverse-complement of fragment end, to simulate a real adapter tail\n",
    "                # on the 3' end in read-space, we append the adapter *as-is* (not RC) to r2seq.\n",
    "                r2seq = r2seq + r2_tail\n",
    "\n",
    "            name = f\"{sample}_{ctg}_{start}\"\n",
    "            r1_records.append((name + \"/1\", r1seq, qual))\n",
    "            r2_records.append((name + \"/2\", r2seq, qual))\n",
    "\n",
    "        return r1_records, r2_records\n",
    "\n",
    "    r1_s1, r2_s1 = generate_pairs(\"sample1\", seqs_s1)\n",
    "    r1_s2, r2_s2 = generate_pairs(\"sample2\", seqs_s2)\n",
    "\n",
    "    # Write gzipped FASTQs\n",
    "    out_r1_s1 = READS_DIR / \"sample1_R1.fastq.gz\"\n",
    "    out_r2_s1 = READS_DIR / \"sample1_R2.fastq.gz\"\n",
    "    out_r1_s2 = READS_DIR / \"sample2_R1.fastq.gz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253d025-d44c-4508-b94e-64cf5e27fdbb",
   "metadata": {},
   "source": [
    "#### Old check environment cell - Completely overdone but a bit cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87b154-5c03-46a2-ad82-f87a190db661",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete overkill but kept here for a rainy day.\n",
    "\n",
    "import os, re, shutil, math\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# -------------------------\n",
    "# Inputs from earlier cells\n",
    "# -------------------------\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    PROJECT_ROOT = \"/group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline\"\n",
    "\n",
    "# These should be set by your selection cell:\n",
    "#   REFERENCE: path to FASTA\n",
    "#   READ_DIR:  selected read directory\n",
    "#   READS:     list of fastq paths\n",
    "#   PAIRS:     dict of paired samples\n",
    "#   FORMAT:    'fastq' or 'fastq.gz'\n",
    "#   results_dir: desired output dir\n",
    "# If not set yet, we still run with Nones.\n",
    "globals_dict = globals()\n",
    "REFERENCE   = globals_dict.get(\"selected_ref\", globals_dict.get(\"REFERENCE\"))\n",
    "READ_DIR    = globals_dict.get(\"selected_read_dir\", globals_dict.get(\"READ_DIR\"))\n",
    "READS       = globals_dict.get(\"selected_reads\", globals_dict.get(\"READS\", []))\n",
    "PAIRS       = globals_dict.get(\"paired_samples\", globals_dict.get(\"PAIRS\", {}))\n",
    "FORMAT      = globals_dict.get(\"read_format\", globals_dict.get(\"FORMAT\"))\n",
    "results_dir = globals_dict.get(\"results_dir\", f\"{PROJECT_ROOT}/results/UNKNOWN_READSET\")\n",
    "\n",
    "# -------------------------\n",
    "# Helpers to read cgroup/Slurm/system limits\n",
    "# -------------------------\n",
    "\n",
    "def _read_first_existing(path_list):\n",
    "    for p in path_list:\n",
    "        try:\n",
    "            with open(p, \"r\") as fh:\n",
    "                return fh.read().strip()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def detect_cpus():\n",
    "    # Start with Python's view\n",
    "    py_cpu = os.cpu_count() or 1\n",
    "\n",
    "    # Slurm envs (common)\n",
    "    slurm_cpus_task = os.getenv(\"SLURM_CPUS_PER_TASK\")\n",
    "    slurm_cpus_node = os.getenv(\"SLURM_CPUS_ON_NODE\")\n",
    "\n",
    "    candidates = [py_cpu]\n",
    "    for v in (slurm_cpus_task, slurm_cpus_node):\n",
    "        try:\n",
    "            if v is not None:\n",
    "                candidates.append(int(v))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # cgroup v2 cpu.max (format: \"quota period\" in microseconds; \"max\" if unlimited)\n",
    "    cpu_max = _read_first_existing([\n",
    "        \"/sys/fs/cgroup/cpu.max\",                       # cgroup v2\n",
    "        \"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\"           # cgroup v1\n",
    "    ])\n",
    "\n",
    "    # cgroup v2: cpu.max \"max <period>\" means no limit; otherwise cpus = quota/period\n",
    "    # cgroup v1: cpu.cfs_quota_us + cpu.cfs_period_us\n",
    "    if cpu_max and \" \" in cpu_max:\n",
    "        quota_str, period_str = cpu_max.split()\n",
    "        if quota_str != \"max\":\n",
    "            try:\n",
    "                quota = int(quota_str); period = int(period_str)\n",
    "                cg_cpus = max(1, int(quota / period))\n",
    "                candidates.append(cg_cpus)\n",
    "            except Exception:\n",
    "                pass\n",
    "    else:\n",
    "        # cgroup v1 pair\n",
    "        quota_v1 = _read_first_existing([\n",
    "            \"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\",\n",
    "        ])\n",
    "        period_v1 = _read_first_existing([\n",
    "            \"/sys/fs/cgroup/cpu/cpu.cfs_period_us\",\n",
    "        ])\n",
    "        try:\n",
    "            if quota_v1 and period_v1:\n",
    "                quota = int(quota_v1); period = int(period_v1)\n",
    "                if quota > 0:\n",
    "                    candidates.append(max(1, int(quota/period)))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return max(1, min(candidates))\n",
    "\n",
    "def detect_mem_bytes():\n",
    "    # Prefer cgroup v2 memory.max (or v1 memory.limit_in_bytes)\n",
    "    val = _read_first_existing([\n",
    "        \"/sys/fs/cgroup/memory.max\",                            # cgroup v2\n",
    "        \"/sys/fs/cgroup/memory/memory.limit_in_bytes\"          # cgroup v1\n",
    "    ])\n",
    "    if val:\n",
    "        try:\n",
    "            # cgroup v2 can return \"max\"\n",
    "            if val.strip() != \"max\":\n",
    "                m = int(val)\n",
    "                # Often memory.high also exists; we stick with hard limit\n",
    "                return m\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fall back to MemTotal from /proc/meminfo\n",
    "    try:\n",
    "        with open(\"/proc/meminfo\", \"r\") as fh:\n",
    "            for line in fh:\n",
    "                if line.startswith(\"MemTotal:\"):\n",
    "                    parts = line.split()\n",
    "                    # MemTotal: <kB>\n",
    "                    kb = int(parts[1])\n",
    "                    return kb * 1024\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def human_bytes(n):\n",
    "    if n is None:\n",
    "        return \"Unknown\"\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\",\"PB\"]\n",
    "    i = 0\n",
    "    f = float(n)\n",
    "    while f >= 1024 and i < len(units)-1:\n",
    "        f/=1024; i+=1\n",
    "    if i <= 2:\n",
    "        return f\"{f:.0f} {units[i]}\"\n",
    "    return f\"{f:.2f} {units[i]}\"\n",
    "\n",
    "def dir_size_bytes(path):\n",
    "    total = 0\n",
    "    for root, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            try:\n",
    "                total += os.path.getsize(os.path.join(root, f))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return total\n",
    "\n",
    "def list_file_sizes(paths):\n",
    "    total = 0\n",
    "    details = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            sz = os.path.getsize(p)\n",
    "            total += sz\n",
    "            details.append((p, sz))\n",
    "        except Exception:\n",
    "            details.append((p, None))\n",
    "    return total, details\n",
    "\n",
    "# -------------------------\n",
    "# Collect environment info\n",
    "# -------------------------\n",
    "\n",
    "cpu_count  = detect_cpus()\n",
    "mem_bytes  = detect_mem_bytes()\n",
    "mem_str    = human_bytes(mem_bytes) if mem_bytes else \"Unknown\"\n",
    "\n",
    "# Slurm environment (if any)\n",
    "slurm_jobid  = os.getenv(\"SLURM_JOB_ID\")\n",
    "slurm_nodel  = os.getenv(\"SLURM_NODELIST\")\n",
    "slurm_cpt    = os.getenv(\"SLURM_CPUS_PER_TASK\")\n",
    "slurm_con    = os.getenv(\"SLURM_CPUS_ON_NODE\")\n",
    "slurm_mem    = os.getenv(\"SLURM_MEM_PER_NODE\") or os.getenv(\"SLURM_MEM_PER_CPU\")\n",
    "\n",
    "# Disk space in results_dir\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "disk = shutil.disk_usage(results_dir)\n",
    "disk_free = disk.free; disk_total = disk.total\n",
    "\n",
    "# Reference and reads sizes\n",
    "ref_size = os.path.getsize(REFERENCE) if REFERENCE and os.path.exists(REFERENCE) else None\n",
    "reads_total, reads_detail = list_file_sizes(READS) if READS else (0, [])\n",
    "\n",
    "# Samples count\n",
    "n_fastq = len(READS) if READS else 0\n",
    "n_pairs = len(PAIRS) if PAIRS else 0\n",
    "\n",
    "# -------------------------\n",
    "# Heuristic guidance\n",
    "# -------------------------\n",
    "problems = []\n",
    "notes = []\n",
    "\n",
    "# Memory heuristics (ballpark):\n",
    "# - bwa-mem/mem2: ~1–2 GB per thread is comfortable for big genomes; spike is modest.\n",
    "# - samtools sort: needs ~ (input BAM size) extra temp; rule of thumb: reserve >= 8–16 GB.\n",
    "# - GATK often benefits from >16 GB; bcftools can work with less.\n",
    "# We don't know BAM size yet; estimate from compressed reads.\n",
    "est_genome_gb = None\n",
    "if REFERENCE and ref_size:\n",
    "    # Genome FASTA disk size isn't equal to genome size (depends on masking/compression),\n",
    "    # but in practice: plain text FASTA bytes ~= number of bases + newlines.\n",
    "    # We'll approximate genome size as ref_size bytes (lower bound).\n",
    "    est_genome_gb = ref_size / (1024**3)\n",
    "\n",
    "reads_gb = reads_total / (1024**3) if reads_total else 0.0\n",
    "\n",
    "# Recommend threads so that mem_per_thread >= 1.5 GB\n",
    "if mem_bytes:\n",
    "    mem_gb = mem_bytes / (1024**3)\n",
    "    recommended_threads_by_mem = max(1, int(mem_gb // 1.5))\n",
    "    recommended_threads = max(1, min(cpu_count, recommended_threads_by_mem))\n",
    "else:\n",
    "    recommended_threads = max(1, cpu_count // 2)  # conservative fallback\n",
    "\n",
    "# Flag low memory relative to big genomes\n",
    "if mem_bytes:\n",
    "    mem_gb = mem_bytes / (1024**3)\n",
    "    if est_genome_gb and est_genome_gb >= 2.0 and mem_gb < 32:\n",
    "        problems.append(\"Memory < 32 GB for a large genome; expect slow or failed sorting/calling.\")\n",
    "    if reads_gb and reads_gb > 10 and mem_gb < 32:\n",
    "        problems.append(\"Many/large reads with < 32 GB RAM; consider fewer threads or chunking.\")\n",
    "else:\n",
    "    notes.append(\"Could not determine memory limit; cgroup/Slurm not detected.\")\n",
    "\n",
    "# Disk space warning\n",
    "if disk_free < 50 * (1024**3):\n",
    "    problems.append(\"Less than 50 GB free in results directory; sorting and VCF steps may run out of space.\")\n",
    "elif disk_free < 20 * (1024**3):\n",
    "    problems.append(\"Less than 20 GB free in results directory; very likely to fail during sort or VCF writing.\")\n",
    "\n",
    "# Pairing expectation\n",
    "if n_fastq > 0 and n_pairs == 0:\n",
    "    problems.append(\"No R1/R2 pairs detected; pipeline expects paired-end data.\")\n",
    "\n",
    "# -------------------------\n",
    "# Render compact summary\n",
    "# -------------------------\n",
    "def row(label, value):\n",
    "    return f\"<div><b>{label}:</b> {value}</div>\"\n",
    "\n",
    "def code(s):\n",
    "    return f\"<code>{s}</code>\"\n",
    "\n",
    "def nice_bool(b):\n",
    "    return \"Yes\" if b else \"No\"\n",
    "\n",
    "html = []\n",
    "html.append(f\"<div style='border:1px solid #444; padding:8px; border-radius:6px; font-size:14px; line-height:1.35;'>\")\n",
    "html.append(\"<div style='font-weight:600; margin-bottom:6px;'>Compute environment</div>\")\n",
    "\n",
    "# CPU/Mem\n",
    "html.append(row(\"CPUs visible\", cpu_count))\n",
    "html.append(row(\"Memory limit\", mem_str))\n",
    "if slurm_jobid:\n",
    "    html.append(row(\"Slurm job\", code(slurm_jobid)))\n",
    "    if slurm_nodel: html.append(row(\"Node(s)\", code(slurm_nodel)))\n",
    "    if slurm_cpt:   html.append(row(\"SLURM_CPUS_PER_TASK\", code(slurm_cpt)))\n",
    "    if slurm_con:   html.append(row(\"SLURM_CPUS_ON_NODE\", code(slurm_con)))\n",
    "    if slurm_mem:   html.append(row(\"SLURM_MEM_*\", code(slurm_mem)))\n",
    "\n",
    "# Disk\n",
    "html.append(row(\"Results dir\", code(results_dir)))\n",
    "html.append(row(\"Disk free / total\", f\"{human_bytes(disk_free)} / {human_bytes(disk_total)}\"))\n",
    "\n",
    "# Data summary\n",
    "html.append(\"<hr style='margin:6px 0;'>\")\n",
    "html.append(\"<div style='font-weight:600; margin-bottom:4px;'>Data summary</div>\")\n",
    "html.append(row(\"Reference\", code(REFERENCE if REFERENCE else 'None')))\n",
    "html.append(row(\"Ref size (bytes)\", f\"{ref_size:,}\" if ref_size is not None else \"Unknown\"))\n",
    "html.append(row(\"Read dir\", code(READ_DIR if READ_DIR else 'None')))\n",
    "html.append(row(\"FASTQ files\", n_fastq))\n",
    "html.append(row(\"Samples (pairs)\", n_pairs))\n",
    "html.append(row(\"Total reads size\", f\"{reads_gb:.2f} GB\"))\n",
    "\n",
    "# Recommendations\n",
    "html.append(\"<hr style='margin:6px 0;'>\")\n",
    "html.append(\"<div style='font-weight:600; margin-bottom:4px;'>Recommendations</div>\")\n",
    "html.append(row(\"Threads suggestion\", f\"{recommended_threads} (limit threads so ~≥1.5 GB RAM per thread)\"))\n",
    "html.append(\"<div style='margin-top:4px; font-size:13px;'>\"\n",
    "            \"• For <b>bwa</b>, set <code>-t {recommended_threads}</code>.<br>\"\n",
    "            \"• For <b>samtools sort</b>, use <code>-@ {recommended_threads}</code> and tune <code>-m</code> (e.g., 1G–2G).<br>\"\n",
    "            \"• For <b>bcftools</b>, modest threading helps; <b>GATK</b> benefits from larger RAM (&ge;16–32 GB).</div>\".format(recommended_threads=recommended_threads))\n",
    "\n",
    "# Problems / Notes\n",
    "if problems:\n",
    "    html.append(\"<hr style='margin:6px 0;'><div style='color:#b71c1c; font-weight:600;'>Warnings</div>\")\n",
    "    html.append(\"<ul style='margin:4px 0; padding-left:18px;'>\" + \"\".join([f\"<li>{p}</li>\" for p in problems]) + \"</ul>\")\n",
    "else:\n",
    "    html.append(\"<p style='color:#2e7d32; margin:6px 0;'><b>All good.</b> Resources look reasonable for a test run.</p>\")\n",
    "\n",
    "if notes:\n",
    "    html.append(\"<div style='margin-top:6px; color:#555;'><b>Notes:</b> \" + \" \".join(notes) + \"</div>\")\n",
    "\n",
    "html.append(\"</div>\")\n",
    "display(HTML(\"\".join(html)))\n",
    "\n",
    "# Expose a variable for downstream cells\n",
    "CPU_THREADS_SUGGESTED = recommended_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1fe5a-6764-411c-a0ad-d59a62d185f8",
   "metadata": {},
   "source": [
    "#### Old overdone selection cell with cool html output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158cd8e-3fe7-421b-a30d-7d2e927c3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cool shit to keep for another time - I like the html bit here - but too long a code cell for most people\n",
    "import os\n",
    "import re\n",
    "from ipywidgets import Dropdown, Button, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- Paths (adjust if you relocate project) ---\n",
    "PROJECT_ROOT = \"/group/jbondgrp2/stephenRichards/_Analysis_projects/_Ems_vcf_pipeline\"\n",
    "refs_dir     = f\"{PROJECT_ROOT}/data/references\"\n",
    "read_dirs_dir= f\"{PROJECT_ROOT}/data/read_directories\"\n",
    "\n",
    "# --- List available references and read directories ---\n",
    "def list_references(path):\n",
    "    # Allow .fa, .fasta, .fna; ignore .fai/.dict etc.\n",
    "    valid_ext = (\".fa\", \".fasta\", \".fna\")\n",
    "    refs = [f for f in os.listdir(path) \n",
    "            if f.lower().endswith(valid_ext) and os.path.isfile(os.path.join(path, f))]\n",
    "    return sorted(refs)\n",
    "\n",
    "def list_read_dirs(path):\n",
    "    return sorted([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))])\n",
    "\n",
    "references = list_references(refs_dir)\n",
    "read_dirs  = list_read_dirs(read_dirs_dir)\n",
    "\n",
    "# --- Widgets ---\n",
    "ref_dropdown      = Dropdown(options=references, description=\"Reference:\", layout={'width':'70%'})\n",
    "read_dir_dropdown = Dropdown(options=read_dirs,  description=\"Read set:\",  layout={'width':'70%'})\n",
    "confirm_button    = Button(description=\"Confirm selection\", button_style=\"success\")\n",
    "msg               = HTML()\n",
    "\n",
    "display(VBox([ref_dropdown, read_dir_dropdown, confirm_button, msg]))\n",
    "\n",
    "# --- Globals populated on confirm ---\n",
    "selected_ref = None\n",
    "selected_read_dir = None\n",
    "selected_reads = []       # all FASTQ files\n",
    "read_format = None        # 'fastq.gz' or 'fastq'\n",
    "paired_samples = {}       # sample_id -> {'R1': path, 'R2': path}\n",
    "unpaired = []             # list of files that could not be paired\n",
    "problems = []             # any issues to report\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "# Recognize R1/R2 patterns and derive a sample ID that groups pairs\n",
    "R1_PATTERNS = [r'(_R?)1(_|\\.|$)', r'(\\.R?)1(_|\\.|$)']\n",
    "R2_PATTERNS = [r'(_R?)2(_|\\.|$)', r'(\\.R?)2(_|\\.|$)']\n",
    "\n",
    "def is_fastq(fname):\n",
    "    low = fname.lower()\n",
    "    return low.endswith(\".fastq\") or low.endswith(\".fq\") or low.endswith(\".fastq.gz\") or low.endswith(\".fq.gz\")\n",
    "\n",
    "def fastq_suffix(fname):\n",
    "    low = fname.lower()\n",
    "    if low.endswith(\".fastq.gz\"): return \"fastq.gz\"\n",
    "    if low.endswith(\".fq.gz\"):     return \"fq.gz\"\n",
    "    if low.endswith(\".fastq\"):     return \"fastq\"\n",
    "    if low.endswith(\".fq\"):        return \"fq\"\n",
    "    return None\n",
    "\n",
    "def detect_read_role_and_stem(basename):\n",
    "    \"\"\"\n",
    "    Return (role, sample_stem) where role in {'R1','R2',None}.\n",
    "    sample_stem is the basename with the R1/R2 token removed to group pairs.\n",
    "    \"\"\"\n",
    "    name = basename\n",
    "    name_no_ext = re.sub(r'(\\.fastq|\\.fq)(\\.gz)?$', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Try R1 first\n",
    "    for pat in R1_PATTERNS:\n",
    "        m = re.search(pat, name_no_ext, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            stem = name_no_ext[:m.start()] + name_no_ext[m.end():]\n",
    "            return \"R1\", stem\n",
    "\n",
    "    # Then R2\n",
    "    for pat in R2_PATTERNS:\n",
    "        m = re.search(pat, name_no_ext, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            stem = name_no_ext[:m.start()] + name_no_ext[m.end():]\n",
    "            return \"R2\", stem\n",
    "\n",
    "    return None, name_no_ext  # role unknown\n",
    "\n",
    "def pair_reads(fastq_files):\n",
    "    \"\"\"\n",
    "    Given a list of fastq file paths, return:\n",
    "      paired: dict sample_id -> {'R1': path, 'R2': path}\n",
    "      unpaired: list of paths that couldn't be paired\n",
    "    Groups by derived sample_stem (see detect_read_role_and_stem).\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "    for f in fastq_files:\n",
    "        base = os.path.basename(f)\n",
    "        role, stem = detect_read_role_and_stem(base)\n",
    "        groups.setdefault(stem, {}).setdefault('files', []).append((role, f))\n",
    "\n",
    "    paired = {}\n",
    "    unpaired_files = []\n",
    "    for stem, info in groups.items():\n",
    "        r1 = [p for role, p in info['files'] if role == 'R1']\n",
    "        r2 = [p for role, p in info['files'] if role == 'R2']\n",
    "\n",
    "        if len(r1) == 1 and len(r2) == 1:\n",
    "            paired[stem] = {'R1': r1[0], 'R2': r2[0]}\n",
    "        else:\n",
    "            # Any files for this stem that aren't a clean pair are considered unpaired\n",
    "            unpaired_files.extend([p for _, p in info['files']])\n",
    "\n",
    "    return paired, sorted(unpaired_files)\n",
    "\n",
    "def summary_html(ref_path, read_dir, fq_files, fmt, paired, unpaired, issues):\n",
    "    n_fastq = len(fq_files)\n",
    "    n_samples = len(paired)\n",
    "    fmt_str = fmt if fmt else \"Unknown\"\n",
    "    problems = list(issues)\n",
    "\n",
    "    if n_fastq == 0:\n",
    "        problems.append(\"No FASTQ files detected in the selected read directory.\")\n",
    "    if fmt is None:\n",
    "        problems.append(\"Read format could not be determined.\")\n",
    "    if unpaired:\n",
    "        problems.append(f\"{len(unpaired)} file(s) could not be paired as R1/R2.\")\n",
    "    if n_samples == 0 and n_fastq > 0:\n",
    "        problems.append(\"No valid R1/R2 pairs detected.\")\n",
    "\n",
    "    color = \"#2e7d32\" if not problems else \"#b71c1c\"\n",
    "\n",
    "    issues_html = \"\"\n",
    "    if problems:\n",
    "        issues_html = \"<ul style='margin:4px 0; padding-left:18px;'>\" + \"\".join([f\"<li>{p}</li>\" for p in problems[:10]]) + \"</ul>\"\n",
    "\n",
    "    return f\"\"\"\n",
    "    <div style=\"border:1px solid {color}; padding:6px; border-radius:6px; font-size:14px; line-height:1.4;\">\n",
    "      <div style=\"margin:0;\">\n",
    "        <b>Reference file:</b> <code>{ref_path}</code><br>\n",
    "        <b>Read directory:</b> <code>{read_dir}</code><br>\n",
    "        <b>Read format:</b> <code>{fmt_str}</code><br>\n",
    "        <b>FASTQ files:</b> {n_fastq} &nbsp;|&nbsp; <b>Samples:</b> {n_samples}\n",
    "      </div>\n",
    "      {\"<hr style='margin:6px 0;'><b>Issues:</b>\" + issues_html if problems else \"<p style='color:#2e7d32; margin:4px 0;'><b>All good.</b> Reads look properly paired.</p>\"}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "def on_confirm(_):\n",
    "    clear_output(wait=True)\n",
    "    display(VBox([ref_dropdown, read_dir_dropdown, confirm_button, msg]))\n",
    "\n",
    "    # Reset globals\n",
    "    global selected_ref, selected_read_dir, selected_reads, read_format, paired_samples, unpaired, problems\n",
    "    selected_ref = os.path.join(refs_dir, ref_dropdown.value) if ref_dropdown.value else None\n",
    "    selected_read_dir = os.path.join(read_dirs_dir, read_dir_dropdown.value) if read_dir_dropdown.value else None\n",
    "    selected_reads = []\n",
    "    read_format = None\n",
    "    paired_samples = {}\n",
    "    unpaired = []\n",
    "    problems = []\n",
    "\n",
    "    # Validate presence\n",
    "    if not selected_ref or not os.path.exists(selected_ref):\n",
    "        problems.append(\"Selected reference file is missing.\")\n",
    "    if not selected_read_dir or not os.path.isdir(selected_read_dir):\n",
    "        problems.append(\"Selected read directory is missing.\")\n",
    "\n",
    "    # Gather FASTQ files in the selected directory\n",
    "    fastqs = []\n",
    "    other_files = []\n",
    "    if selected_read_dir and os.path.isdir(selected_read_dir):\n",
    "        for f in sorted(os.listdir(selected_read_dir)):\n",
    "            path = os.path.join(selected_read_dir, f)\n",
    "            if os.path.isfile(path):\n",
    "                if is_fastq(f):\n",
    "                    fastqs.append(path)\n",
    "                else:\n",
    "                    other_files.append(path)\n",
    "    selected_reads = fastqs\n",
    "\n",
    "    # Determine read format consistency\n",
    "    exts = {fastq_suffix(os.path.basename(p)) for p in fastqs}\n",
    "    exts.discard(None)\n",
    "    if len(exts) == 0:\n",
    "        read_format = None\n",
    "    elif len(exts) == 1:\n",
    "        only = exts.pop()\n",
    "        # Normalize fq vs fastq\n",
    "        read_format = \"fastq.gz\" if \"gz\" in only else \"fastq\"\n",
    "    else:\n",
    "        problems.append(f\"Mixed FASTQ extensions detected: {', '.join(sorted(exts))}. Please standardize.\")\n",
    "        # Best guess for display\n",
    "        read_format = \"/\".join(sorted(exts))\n",
    "\n",
    "    # Pair reads\n",
    "    paired_samples, unpaired = pair_reads(fastqs)\n",
    "    # Build and display summary\n",
    "    msg.value = summary_html(selected_ref, selected_read_dir, fastqs, read_format, paired_samples, unpaired, problems)\n",
    "    \n",
    "confirm_button.on_click(on_confirm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c674d-1a73-4ce8-b438-71b35f9e11e3",
   "metadata": {},
   "source": [
    "#### Old bcftools multisample caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95938be2-21e0-4b7e-ab67-8f18f59e2357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Variant calling with bcftools (multisample)\n",
    "import os, shutil, subprocess, glob\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------- Inputs expected -----------\n",
    "try:\n",
    "    PROJECT_ROOT\n",
    "except NameError:\n",
    "    raise RuntimeError(\"PROJECT_ROOT not set. Run earlier setup cells.\")\n",
    "\n",
    "READ_SET_NAME = globals().get(\"READ_SET_NAME\", None) or read_dir_dropdown.value\n",
    "RUN_DIR    = Path(PROJECT_ROOT) / \"results\" / READ_SET_NAME\n",
    "ALN_DIR    = RUN_DIR / \"alignments\"\n",
    "VAR_DIR    = RUN_DIR / \"variants\" / \"bcftools\"\n",
    "LOGS_DIR   = RUN_DIR / \"logs\"\n",
    "for d in (VAR_DIR, LOGS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# From indexing cell:\n",
    "try:\n",
    "    REF_FASTA\n",
    "except NameError:\n",
    "    raise RuntimeError(\"REF_FASTA is not defined. Run the indexing cell first.\")\n",
    "\n",
    "# Optional threads from compute cell\n",
    "CPU_THREADS_SUGGESTED = globals().get(\"CPU_THREADS_SUGGESTED\", None) or max(1, (os.cpu_count() or 2)//2)\n",
    "THREADS = max(1, min(int(CPU_THREADS_SUGGESTED), 16))\n",
    "\n",
    "def have_tool(name: str) -> bool:\n",
    "    return shutil.which(name) is not None\n",
    "\n",
    "def run_pipe_bcftools_mpileup_call(bams, out_vcf_gz, log_path):\n",
    "    \"\"\"Run: bcftools mpileup | bcftools call, writing bgzipped VCF.\"\"\"\n",
    "    mpileup_cmd = [\n",
    "        \"bcftools\", \"mpileup\",\n",
    "        \"-Ou\",              # uncompressed BCF to stdout\n",
    "        \"-f\", str(REF_FASTA),\n",
    "        \"-q\", \"20\",         # min MAPQ\n",
    "        \"-Q\", \"20\",         # min base quality\n",
    "        \"-a\", \"FORMAT/DP,FORMAT/AD\",  # carry per-sample depth/allele depth\n",
    "    ] + [str(b) for b in bams]\n",
    "\n",
    "    call_cmd = [\"bcftools\", \"call\", \"-mv\", \"-Oz\", \"-o\", str(out_vcf_gz)]\n",
    "\n",
    "    print(\"  $\", \" \".join(mpileup_cmd), \"|\", \" \".join(call_cmd))\n",
    "    with open(log_path, \"w\") as lf:\n",
    "        lf.write(\"CMD: \" + \" \".join(mpileup_cmd) + \" | \" + \" \".join(call_cmd) + \"\\n\\n\")\n",
    "        # Start mpileup\n",
    "        p1 = subprocess.Popen(mpileup_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=False)\n",
    "        # Pipe into call\n",
    "        p2 = subprocess.Popen(call_cmd, stdin=p1.stdout, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=False)\n",
    "        p1.stdout.close()\n",
    "\n",
    "        out2, err2 = p2.communicate()\n",
    "        out1, err1 = p1.communicate()\n",
    "\n",
    "        lf.write(\"MPILEUP STDERR:\\n\" + (err1.decode('utf-8', errors='ignore') if isinstance(err1, (bytes, bytearray)) else str(err1)) + \"\\n\\n\")\n",
    "        lf.write(\"CALL STDERR:\\n\"    + (err2.decode('utf-8', errors='ignore') if isinstance(err2, (bytes, bytearray)) else str(err2)) + \"\\n\")\n",
    "\n",
    "        if p1.returncode != 0 or p2.returncode != 0:\n",
    "            raise RuntimeError(f\"bcftools pipeline failed. See log: {log_path}\")\n",
    "\n",
    "def run_cmd(cmd, log_path=None):\n",
    "    print(\"  $\", \" \".join(cmd))\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if log_path:\n",
    "        with open(log_path, \"a\") as lf:\n",
    "            lf.write(\"\\n\" + \" \".join(cmd) + \"\\n\")\n",
    "            lf.write(\"STDOUT:\\n\" + (res.stdout or \"\") + \"\\n\")\n",
    "            lf.write(\"STDERR:\\n\" + (res.stderr or \"\") + \"\\n\")\n",
    "    else:\n",
    "        if res.stdout.strip(): print(res.stdout.strip())\n",
    "        if res.stderr.strip(): print(res.stderr.strip())\n",
    "    if res.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {' '.join(cmd)}\")\n",
    "\n",
    "# Tool checks\n",
    "for tool in (\"bcftools\", \"samtools\"):\n",
    "    if not have_tool(tool):\n",
    "        raise RuntimeError(f\"{tool} not found in PATH. Activate the conda env used for this notebook.\")\n",
    "\n",
    "# Find BAMs\n",
    "bams = sorted(Path(ALN_DIR).glob(\"*.dedup.bam\"))\n",
    "if not bams:\n",
    "    raise RuntimeError(f\"No deduplicated BAMs found in {ALN_DIR}. Run the alignment cell first.\")\n",
    "\n",
    "# Ensure BAM indices exist\n",
    "for b in bams:\n",
    "    bai = Path(str(b) + \".bai\")\n",
    "    if not bai.exists():\n",
    "        print(f\"Indexing BAM: {b.name}\")\n",
    "        run_cmd([\"samtools\", \"index\", str(b)])\n",
    "\n",
    "# Output paths\n",
    "out_vcf_gz = VAR_DIR / \"cohort.bcftools.vcf.gz\"\n",
    "out_vcf_tbi = VAR_DIR / \"cohort.bcftools.vcf.gz.tbi\"\n",
    "log_path = LOGS_DIR / \"bcftools_call.log\"\n",
    "\n",
    "# Idempotency check\n",
    "if out_vcf_gz.exists() and out_vcf_tbi.exists():\n",
    "    print(f\"bcftools VCF already exists: {out_vcf_gz}\")\n",
    "else:\n",
    "    print(f\"Running bcftools (threads: {THREADS}) on {len(bams)} BAM(s)\")\n",
    "    # bcftools uses OpenMP/pthreads internally; we can hint via OMP/MKL vars (optional)\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(THREADS)\n",
    "    os.environ[\"OPENBLAS_NUM_THREADS\"] = str(THREADS)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(THREADS)\n",
    "\n",
    "    run_pipe_bcftools_mpileup_call(bams, out_vcf_gz, log_path)\n",
    "\n",
    "    # Index the VCF\n",
    "    print(\"Indexing VCF...\")\n",
    "    run_cmd([\"bcftools\", \"index\", \"-t\", str(out_vcf_gz)], log_path=log_path)\n",
    "\n",
    "# Optional: basic stats\n",
    "stats_txt = VAR_DIR / \"cohort.bcftools.stats.txt\"\n",
    "if not stats_txt.exists():\n",
    "    print(\"Computing bcftools stats...\")\n",
    "    run_cmd([\"bcftools\", \"stats\", \"-F\", str(REF_FASTA), \"-s\", \"-\", str(out_vcf_gz)])\n",
    "    # bcftools stats prints to stdout; we can save separately if desired\n",
    "\n",
    "print(\"\\nbcftools calling complete.\")\n",
    "print(\"Output:\", out_vcf_gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5415a-20f8-4c09-bb76-c3c37d486ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Bondlab_phylo_env]",
   "language": "python",
   "name": "conda-env-Bondlab_phylo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
